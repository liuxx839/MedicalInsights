#main.py
import streamlit as st
from layout import setup_layout
from functions import (
    setup_client, generate_tag, generate_diseases_tag, rewrite,
    prob_identy, generate_structure_data
)
from config import (
    topics, diseases, institutions, departments, persons,
    primary_topics_list, primary_diseases_list, colors
)
from streamlit_extras.stylable_container import stylable_container
import pandas as pd
import json
import time
import ast
import io
from openai import OpenAI
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from io import BytesIO
from dagrelation import DAGRelations
from datadescription import DataDescription
import numpy as np
from datetime import datetime
from prophet import Prophet
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import base64
import re
import os

# æ–°å¢çš„å¯¼å…¥ï¼ˆåŸæ¥æ²¡æœ‰ï¼‰
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from sklearn.neighbors import NearestNeighbors
from scipy.stats import chi2_contingency
import matplotlib as mpl

# Set Matplotlib font configuration
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'WenQuanYi Zen Hei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False


model_choice_research, client_research = setup_client(model_choice = 'gemini-2.0-flash')
 
def create_mermaid_html_from_edges(dag_edges):
    """
    Create a Mermaid flowchart HTML from a list of edges.
    
    Parameters:
    dag_edges (list): List of tuples representing edges. Each tuple can be:
                     - ('var1', 'var2') for a single edge
                     - (['var1', 'var2'], 'var3') for multiple sources to one target
    
    Returns:
    str: HTML code with embedded Mermaid flowchart
    """
    mermaid_code = ['flowchart TD']
    
    for edge in dag_edges:
        if isinstance(edge[0], list):
            # Handle multiple sources to one target
            source_vars = edge[0]
            target_var = edge[1]
            
            # Create individual connections for each source to target
            for source_var in source_vars:
                mermaid_code.append(f'    {source_var} --> {target_var}')
        else:
            # Simple one-to-one edge
            source_var = edge[0]
            target_var = edge[1]
            mermaid_code.append(f'    {source_var} --> {target_var}')
    
    # Join all lines with newlines
    mermaid_code_str = '\n'.join(mermaid_code)
    
    # Create the HTML with the embedded Mermaid code and zoom controls
    mermaid_html = f"""
<div id="mermaid-container" style="width:100%; height:100%; position:relative;">
    <div class="mermaid" id="mermaid-diagram">
    {mermaid_code_str}
    </div>
    
    <div style="position:absolute; top:10px; right:10px; background:white; padding:5px; border:1px solid #ccc; border-radius:5px;">
        <button onclick="zoomIn()" style="margin-right:5px;">â•</button>
        <button onclick="zoomOut()">â–</button>
        <button onclick="resetZoom()" style="margin-left:5px;">ğŸ”„</button>
    </div>
</div>

<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({{ startOnLoad: true }});
    
    window.currentZoom = 1;
    
    window.zoomIn = function() {{
        window.currentZoom += 0.1;
        document.getElementById('mermaid-diagram').style.transform = `scale(${{window.currentZoom}})`;
        document.getElementById('mermaid-diagram').style.transformOrigin = 'top left';
    }};
    
    window.zoomOut = function() {{
        if (window.currentZoom > 0.5) {{
            window.currentZoom -= 0.1;
            document.getElementById('mermaid-diagram').style.transform = `scale(${{window.currentZoom}})`;
            document.getElementById('mermaid-diagram').style.transformOrigin = 'top left';
        }}
    }};
    
    window.resetZoom = function() {{
        window.currentZoom = 1;
        document.getElementById('mermaid-diagram').style.transform = 'scale(1)';
    }};
</script>
"""
    
    return mermaid_html
    
def create_word_document(qa_response, fact_check_result=None):
    """
    åˆ›å»ºåŒ…å«QAå“åº”å’Œäº‹å®æ ¸æŸ¥ç»“æœçš„Wordæ–‡æ¡£ï¼Œæ”¯æŒMarkdownæ ‡é¢˜è½¬æ¢ä¸ºWordæ ·å¼
    """
    doc = Document()
    
    # è®¾ç½®æ ‡é¢˜
    title = doc.add_heading('Medical Knowledge Base Q&A Report', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # æ·»åŠ æ—¶é—´æˆ³
    timestamp = doc.add_paragraph()
    timestamp.add_run(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}").italic = True
    
    # æ·»åŠ QAå“åº”
    doc.add_heading('å›åº”å†…å®¹', level=1)
    
    # å¤„ç†QAå“åº”ä¸­çš„Markdownæ ¼å¼
    lines = qa_response.split('\n')
    current_text = []
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
        if line.strip().startswith('#'):
            # å¦‚æœä¹‹å‰æœ‰ç´¯ç§¯çš„æ–‡æœ¬ï¼Œå…ˆæ·»åŠ ä¸ºæ®µè½
            if current_text:
                doc.add_paragraph(''.join(current_text))
                current_text = []
            
            # è®¡ç®—æ ‡é¢˜çº§åˆ«
            level = 1
            line = line.strip()
            while line.startswith('#'):
                level += 1
                line = line[1:]
            level = min(level, 9)  # Wordæ”¯æŒæœ€å¤š9çº§æ ‡é¢˜
            
            # ç§»é™¤å¯èƒ½çš„å¼•ç”¨æ ‡è®° [1,2,3] å¹¶ä¿å­˜
            citation = ''
            if '[' in line and ']' in line:
                main_text = line[:line.find('[')].strip()
                citation = line[line.find('['):].strip()
                heading = doc.add_heading(main_text, level=level-1)
                if citation:
                    heading.add_run(f" {citation}").italic = True
            else:
                doc.add_heading(line.strip(), level=level-1)
        else:
            current_text.append(line + '\n')
    
    # æ·»åŠ æœ€åå‰©ä½™çš„æ–‡æœ¬
    if current_text:
        doc.add_paragraph(''.join(current_text))
    
    # å¦‚æœæœ‰äº‹å®æ ¸æŸ¥ç»“æœï¼Œæ·»åŠ åˆ°æ–‡æ¡£
    if fact_check_result:
        doc.add_heading('äº‹å®æ ¸æŸ¥ç»“æœ', level=1)
        # å¯¹äº‹å®æ ¸æŸ¥ç»“æœä¹Ÿè¿›è¡Œç›¸åŒçš„å¤„ç†
        lines = fact_check_result.split('\n')
        current_text = []
        
        for line in lines:
            if line.strip().startswith('#'):
                if current_text:
                    doc.add_paragraph(''.join(current_text))
                    current_text = []
                
                level = 1
                line = line.strip()
                while line.startswith('#'):
                    level += 1
                    line = line[1:]
                level = min(level, 9)
                
                if '[' in line and ']' in line:
                    main_text = line[:line.find('[')].strip()
                    citation = line[line.find('['):].strip()
                    heading = doc.add_heading(main_text, level=level-1)
                    if citation:
                        heading.add_run(f" {citation}").italic = True
                else:
                    doc.add_heading(line.strip(), level=level-1)
            else:
                current_text.append(line + '\n')
        
        if current_text:
            doc.add_paragraph(''.join(current_text))
    
    # ä¿å­˜åˆ°å†…å­˜ä¸­
    doc_io = io.BytesIO()
    doc.save(doc_io)
    doc_io.seek(0)
    return doc_io

def extract_dag_edges(text_content):
    # ä»æ–‡æœ¬ä¸­æå– dag_edges éƒ¨åˆ†
    if "dag_edges = [" in text_content:
        start_idx = text_content.find("dag_edges = [")
        start_idx = text_content.find("[", start_idx)
        
        # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·
        open_brackets = 1
        end_idx = start_idx + 1
        
        while open_brackets > 0 and end_idx < len(text_content):
            if text_content[end_idx] == '[':
                open_brackets += 1
            elif text_content[end_idx] == ']':
                open_brackets -= 1
            end_idx += 1
        
        # æå– dag_edges åˆ—è¡¨å†…å®¹
        dag_edges_str = text_content[start_idx:end_idx]
        
        # ä½¿ç”¨ ast.literal_eval å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º Python å¯¹è±¡
        try:
            dag_edges = ast.literal_eval(dag_edges_str)
            return dag_edges
        except (SyntaxError, ValueError) as e:
            print(f"è§£æé”™è¯¯: {e}")
            return None
    
    return None

def setup_spreadsheet_analysis():
    # st.markdown(
    #     """
    # <h1 style='text-align: center; font-size: 18px; font-weight: bold;'>Spreadsheet Analysis</h1>
    # <h6 style='text-align: center; font-size: 12px;'>ä¸Šä¼ Excel/CSVæ–‡ä»¶æˆ–ç²˜è´´JSONæ•°æ®è¿›è¡Œåˆ†æ</h6>
    # <br><br><br>
    # """,
    #     unsafe_allow_html=True,
    # )
    
    # Initialize all session state variables
    if "analysis_response" not in st.session_state:
        st.session_state.analysis_response = ""
    if "analysis_reasoning" not in st.session_state:
        st.session_state.analysis_reasoning = ""
    if "dag_edges" not in st.session_state:
        st.session_state.dag_edges = ""
    if "sample_data" not in st.session_state:
        st.session_state.sample_data = {}
    if "business_report" not in st.session_state:
        st.session_state.business_report = ""
    if "dag_report" not in st.session_state:
        st.session_state.dag_report = ""
    if "dag_reasoning" not in st.session_state:
        st.session_state.dag_reasoning = ""
    if "df" not in st.session_state:
        st.session_state.df = None
    
    # Create tabs for different input methods
    tab1, tab2 = st.tabs(["æ–‡ä»¶ä¸Šä¼ ", "JSONç²˜è´´"])
    
    with tab1:
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶(xlsx/csv)", type=['xlsx', 'csv'])
        
        if uploaded_file is not None:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ•°æ®
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                    
                # å¤„ç†åˆ—åï¼Œå°†ç©ºæ ¼å’Œç‰¹æ®Šç¬¦å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
                df.columns = [re.sub(r'[^\w]', '_', col) for col in df.columns]
                
                # ä¿å­˜DataFrameåˆ°session state
                st.session_state.df = df
                
                # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
                st.write("æ•°æ®é¢„è§ˆ:")
                st.dataframe(df)
                
                # ä¿å­˜sampleæ•°æ®åˆ°session state
                st.session_state.sample_data = df.sample(10).to_dict()
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    with tab2:
        json_input = st.text_area("ç²˜è´´JSONæ•°æ®:", height=200)
        process_json_button = st.button("å¤„ç†JSONæ•°æ®")
        
        if process_json_button and json_input:
            try:
                # æ¸…ç†JSONå­—ç¬¦ä¸² - å¦‚æœå‰åæœ‰é¢å¤–çš„å¼•å·ï¼Œå»æ‰å®ƒä»¬
                cleaned_json = json_input.strip()
                if cleaned_json.startswith('"') and cleaned_json.endswith('"'):
                    # å¦‚æœJSONè¢«é¢å¤–çš„å¼•å·åŒ…å›´ï¼Œå»æ‰è¿™äº›å¼•å·å¹¶å¤„ç†è½¬ä¹‰å­—ç¬¦
                    cleaned_json = cleaned_json[1:-1].replace('\\"', '"')
                
                # è§£æJSONæ•°æ®
                try:
                    json_data = json.loads(cleaned_json)
                    df = pd.DataFrame(json_data)
                except Exception as e:
                    # å¦‚æœç¬¬ä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œå°è¯•ä»¥Recordsæ ¼å¼è§£æ
                    try:
                        json_data = json.loads(cleaned_json)
                        if isinstance(json_data, list):
                            df = pd.DataFrame(json_data)
                        else:
                            df = pd.DataFrame([json_data])
                    except:
                        st.error(f"æ— æ³•è§£æJSONæ•°æ®: {str(e)}")
                        return
                
                # ä¿å­˜DataFrameåˆ°session state
                st.session_state.df = df
                
                # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
                st.write("æ•°æ®é¢„è§ˆ:")
                st.dataframe(df)
                
                # ä¿å­˜sampleæ•°æ®åˆ°session state
                st.session_state.sample_data = df.head(10).to_dict()
                
            except Exception as e:
                st.error(f"å¤„ç†JSONæ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    # åªæœ‰å½“DataFrameå¯ç”¨æ—¶æ‰æ˜¾ç¤ºä¸‹é¢çš„æ§ä»¶
    if st.session_state.df is not None:
        # ç”¨æˆ·è¾“å…¥éœ€æ±‚
        user_question = st.text_area("è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚:", height=100)
        
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼Œä¸€åˆ—æ”¾ç”ŸæˆæŒ‰é’®ï¼Œä¸€åˆ—æ”¾DAGåˆ†ææŒ‰é’®
        col1, col2 = st.columns(2)
        
        with col1:
            # æ ¹æ®æ˜¯å¦å·²ç»è¿›è¡Œè¿‡DAGåˆ†ææ¥æ˜¾ç¤ºä¸åŒçš„æŒ‰é’®æ–‡æœ¬
            button_text = "å†æ¬¡ç”Ÿæˆ" if "business_report" in st.session_state and st.session_state.business_report else "ç”Ÿæˆ"
            generate_button = st.button(button_text)
        
        with col2:
            with stylable_container(
                "dag_analysis_button",
                css_styles="""
                    button {
                        background-color: #FFA500;
                        color: white;
                    }""",
            ):
                dag_analysis_button = st.button("ğŸ“Š DAGåˆ†æ")
        
        # ç¡®ä¿æœ‰DataFrameå¯ç”¨äºåˆ†æ
        df = st.session_state.df
        
        # å¤„ç†ç”ŸæˆæŒ‰é’®ç‚¹å‡»
        if generate_button and user_question:
            with st.spinner("æ­£åœ¨åˆ†æ..."):
                # æ ¹æ®æ˜¯å¦å·²ç»æœ‰business_reportæ¥å†³å®šä½¿ç”¨å“ªä¸ªæç¤º
                if "business_report" in st.session_state and st.session_state.business_report:
                    # ä½¿ç”¨å·²æœ‰çš„business_reportä½œä¸ºè¾“å…¥ï¼Œé‡æ–°æ€è€ƒDAGç»“æ„
                    response = client_research.chat.completions.create(
                        model=model_choice_research,
                        messages=[
                            {
                                "role": "system",
                                "content": """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„sampleæ•°æ®å’Œå·²æœ‰çš„åˆ†ææŠ¥å‘Šï¼Œé‡æ–°æ€è€ƒå¹¶ä¼˜åŒ–DAGå…³ç³»ã€‚
                                å…³æ³¨ä»¥ä¸‹å‡ ç‚¹ï¼š
                                1. ä»”ç»†åˆ†æå·²æœ‰æŠ¥å‘Šä¸­çš„å‘ç°å’Œå…³ç³»
                                2. é‡æ–°è¯„ä¼°å¯èƒ½çš„å› æœå…³ç³»
                                3. æ„å»ºæ›´ä¼˜åŒ–çš„DAGè¾¹
                                4. æ”¯æŒå¤šå¯¹ä¸€çš„å…³ç³»
                                5. ç¡®ä¿ä½¿ç”¨çš„æ˜¯åŸå§‹çš„åˆ—åï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹
                                6. è€ƒè™‘å·²æœ‰åˆ†æä¸­å¯èƒ½è¢«å¿½ç•¥çš„å…³ç³»
                                7. ä¸è¦å¢åŠ ä»»ä½•ä¸å­˜åœ¨çš„åˆ—å

                                è¯·å…ˆæä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œç„¶åå†ç»™å‡ºä¼˜åŒ–åçš„DAGå®šä¹‰ã€‚

                                æœ€ç»ˆè¾“å‡ºæ ¼å¼å¿…é¡»å¦‚ä¸‹ï¼š
                                ##æ¨ç†è¿‡ç¨‹
                                [è¯¦ç»†çš„åˆ†ææ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¯¹å·²æœ‰åˆ†æçš„è¯„ä¼°]

                                ##å®šä¹‰DAGè¾¹ï¼ˆæ”¯æŒå¤šå¯¹ä¸€å…³ç³»ï¼‰
                                è¯·ä¸¥æ ¼éµå¾ªä¸‹é¢çš„æ ¼å¼ï¼Œä»”ç»†æ ¸å¯¹ï¼ŒåŠ¡å¿…ä¿è¯ä½¿ç”¨åŸå§‹åˆ—åï¼Œä»¥åŠæ­£ç¡®çš„æ‹¬å·
                                dag_edges = [
                                    ('var1', 'var2'),
                                    ('var3', 'var4'),
                                    # å¤šå¯¹ä¸€å…³ç³»ç¤ºä¾‹
                                    (['var1', 'var2'], 'var3'),
                                    (['var1', 'var2', 'var3'], 'var4')
                                ]
                                
                                ##åˆ†æè¯´æ˜ï¼š
                                [è¿™é‡Œæ˜¯å¯¹ä¼˜åŒ–åDAGç»“æ„çš„è§£é‡Šè¯´æ˜ï¼Œä»¥åŠä¸ä¹‹å‰åˆ†æçš„æ¯”è¾ƒ]
                                """
                            },
                            {
                                "role": "user",
                                "content": f"Sampleæ•°æ®ï¼š\n{st.session_state.sample_data}\n\nç”¨æˆ·éœ€æ±‚ï¼š{user_question}\n\nå·²æœ‰åˆ†ææŠ¥å‘Šï¼š\n{st.session_state.business_report}"
                            }
                        ],
                        temperature=0.7,
                        max_tokens=2000,
                        stream=True
                    )
                else:
                    # åŸæœ‰çš„æç¤ºï¼Œç”¨äºé¦–æ¬¡ç”Ÿæˆ
                    response = client_research.chat.completions.create(
                        model=model_choice_research,
                        messages=[
                            {
                                "role": "system", 
                                "content": """
                                ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„sampleæ•°æ®åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œå¹¶æ„å»ºæ½œåœ¨çš„DAGå…³ç³»ã€‚
                                å…³æ³¨ä»¥ä¸‹å‡ ç‚¹ï¼š
                                1. ä»”ç»†åˆ†ææ•°æ®åˆ—ä¹‹é—´çš„å…³ç³»
                                2. è¯†åˆ«å¯èƒ½çš„å› æœå…³ç³»
                                3. æ„å»ºåˆé€‚çš„DAGè¾¹
                                4. æ”¯æŒå¤šå¯¹ä¸€çš„å…³ç³»
                                5. ç¡®ä¿ä½¿ç”¨çš„æ˜¯åŸå§‹çš„åˆ—åï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹
                                6. ä¸è¦å¢åŠ ä»»ä½•ä¸å­˜åœ¨çš„åˆ—å

                                æœ€ç»ˆè¾“å‡ºæ ¼å¼å¿…é¡»å¦‚ä¸‹ï¼š

                                ## æ¨ç†è¿‡ç¨‹
                                [è¯¦ç»†çš„åˆ†ææ¨ç†è¿‡ç¨‹]

                                ## å®šä¹‰DAGè¾¹ï¼ˆæ”¯æŒå¤šå¯¹ä¸€å…³ç³»ï¼‰
                                è¯·ä¸¥æ ¼éµå¾ªä¸‹é¢çš„æ ¼å¼ï¼Œä»”ç»†æ ¸å¯¹ï¼ŒåŠ¡å¿…ä¿è¯ä½¿ç”¨åŸå§‹åˆ—åï¼Œä»¥åŠæ­£ç¡®çš„æ‹¬å·
                                dag_edges = [
                                    ('var1', 'var2'),
                                    ('var3', 'var4'),
                                    # å¤šå¯¹ä¸€å…³ç³»ç¤ºä¾‹
                                    (['var1', 'var2'], 'var3'),
                                    (['var1', 'var2', 'var3'], 'var4')
                                ]


                                ## åˆ†æè¯´æ˜ï¼š
                                [è¿™é‡Œæ˜¯å¯¹DAGç»“æ„çš„è§£é‡Šè¯´æ˜ï¼ŒåŒ…å«åŸåˆ™éµå¾ªæƒ…å†µåˆ†æ]
                                """
                            },
                            {
                                "role": "user", 
                                "content": f"Sampleæ•°æ®ï¼š\n{st.session_state.sample_data}\n\nç”¨æˆ·éœ€æ±‚ï¼š{user_question}"
                            }
                        ],
                        temperature=0.7,
                        max_tokens=2000,
                        stream=True
                    )
                
                full_response = ""
                reasoning_content = ""
                
                # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçš„å®¹å™¨
                progress_container = st.empty()
                
                # å¤„ç†æµå¼å“åº”
                for chunk in response:
                    if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                        reasoning_content += chunk.choices[0].delta.reasoning_content
                        progress_container.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nå›ç­”ï¼š\n{full_response}")
                    elif hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        progress_container.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nå›ç­”ï¼š\n{full_response}")
                
                # ä»…æå–DAGå®šä¹‰éƒ¨åˆ†
                dag_definition = ""
                if "dag_edges = [" in full_response:
                    dag_start = full_response.find("dag_edges = [")
                    
                    # Find the matching closing bracket by counting opening and closing brackets
                    open_brackets = 1
                    dag_end = dag_start + len("dag_edges = [")
                    
                    while open_brackets > 0 and dag_end < len(full_response):
                        if full_response[dag_end] == '[':
                            open_brackets += 1
                        elif full_response[dag_end] == ']':
                            open_brackets -= 1
                        dag_end += 1
                    
                    dag_definition = full_response[dag_start:dag_end]
                
                # ä¿å­˜åˆ°session state
                st.session_state.analysis_response = full_response
                st.session_state.analysis_reasoning = reasoning_content
                st.session_state.dag_edges = dag_definition
                
                # æ¸…ç©ºè¿›åº¦å®¹å™¨
                progress_container.empty()
        
        # å¤„ç†DAGåˆ†ææŒ‰é’®ç‚¹å‡»
        if dag_analysis_button and st.session_state.dag_edges:
            with st.spinner("æ­£åœ¨è¿›è¡ŒDAGåˆ†æ..."):
                try:
                    # æ‰§è¡ŒDAGåˆ†æ
                    dag_edges_text = st.session_state.dag_edges
                    dag_edges = extract_dag_edges(dag_edges_text)
                    
                    full_response = ""
                    reasoning_content = ""

                    # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçš„å®¹å™¨
                    dag_progress = st.empty()
                    
                    if dag_edges:
                        # æ‰§è¡ŒDAGåˆ†æ
                        analyzer = DAGRelations(df, dag_edges)
                        dag_report = analyzer.analyze_relations().print_report()
                        st.session_state.dag_report = dag_report

                        # æ·»åŠ æ•°æ®æè¿°åˆ†æ
                        data_analyzer = DataDescription(df, include_histogram=False, string_threshold=10)
                        data_analyzer.analyze_data()
                        json_output = data_analyzer.to_json()
                        st.session_state.data_description = json_output

                        # ç”Ÿæˆå•†ä¸šæŠ¥å‘Š
                        response = client_research.chat.completions.create(
                            model=model_choice_research,
                            messages=[
                                {
                                    "role": "system",
                                    "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·åŸºäºæä¾›çš„åˆå§‹åˆ†æç»“æœã€DAGåˆ†ææŠ¥å‘Šå’Œæ•°æ®æè¿°ä¿¡æ¯ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚å¦‚æœDAGåˆ†ææŠ¥å‘Šä¸å­˜åœ¨ï¼Œè¯·åœ¨æŠ¥å‘Šä¸­è¯´æ˜è¿™ä¸€æƒ…å†µã€‚

                                        æŠ¥å‘Šç»“æ„åº”åŒ…å«ï¼š

                                        # æ‰§è¡Œæ‘˜è¦
                                        ç®€æ˜æ‰¼è¦åœ°æ€»ç»“å…³é”®å‘ç°å’Œå»ºè®®ï¼ˆä¸è¶…è¿‡200å­—ï¼‰
                                        
                                        # å…³é”®æ´å¯Ÿ
                                        ## ä¸»è¦å‘ç°
                                        - å‘ç°1: [ç®€æ˜æè¿°] - æ”¯æŒæ•°æ®: [ç›¸å…³ç»Ÿè®¡ç»“æœ]
                                        - å‘ç°2: [ç®€æ˜æè¿°] - æ”¯æŒæ•°æ®: [ç›¸å…³ç»Ÿè®¡ç»“æœ]
                                        - å‘ç°n: ...
                                        
                                        # æ•°æ®æ¦‚è§ˆ
                                        ## æ•°æ®åŸºæœ¬æƒ…å†µ
                                        ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ï¼š
                                        | æ•°æ®ç»´åº¦ | å€¼ |
                                        | --- | --- |
                                        | æ€»è®°å½•æ•° | X |
                                        | å˜é‡æ•°é‡ | X |
                                        | æ•°å€¼å‹å˜é‡ | Xä¸ª (åˆ—å‡ºåç§°) |
                                        | åˆ†ç±»å‹å˜é‡ | Xä¸ª (åˆ—å‡ºåç§°) |
                                        | ç¼ºå¤±å€¼æƒ…å†µ | æ€»ä½“ç™¾åˆ†æ¯”åŠä¸»è¦ç¼ºå¤±åˆ— |

                                        ## å…³é”®å˜é‡åˆ†æ
                                        é’ˆå¯¹é‡è¦å˜é‡ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ï¼š

                                        **æ•°å€¼å‹å˜é‡ç»Ÿè®¡**
                                        | å˜é‡å | å‡å€¼ | ä¸­ä½æ•° | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | å¼‚å¸¸å€¼æ¯”ä¾‹ |
                                        | --- | --- | --- | --- | --- | --- | --- |

                                        **åˆ†ç±»å‹å˜é‡ç»Ÿè®¡**
                                        | å˜é‡å | å”¯ä¸€å€¼æ•°é‡ | æœ€å¸¸è§ç±»åˆ«(å æ¯”) | ç†µå€¼(å½’ä¸€åŒ–) | åˆ†å¸ƒå‡è¡¡åº¦ |
                                        | --- | --- | --- | --- | --- | --- |

                                        # å˜é‡å…³ç³»åˆ†æ
                                        ## DAGå…³ç³»æ¦‚è¿°
                                        ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ä¸»è¦å˜é‡é—´çš„å…³ç³»ï¼š

                                        | å…³ç³»ç±»å‹ | æºå˜é‡ | ç›®æ ‡å˜é‡ | ç»Ÿè®¡é‡ | på€¼ | æ˜¾è‘—æ€§ | å…³ç³»å¼ºåº¦ |
                                        | --- | --- | --- | --- | --- | --- | --- |

                                        ## è¯¦ç»†å…³ç³»åˆ†æ
                                        é’ˆå¯¹æ¯ä¸ªé‡è¦å…³ç³»ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

                                        ### å…³ç³»: [æºå˜é‡] -> [ç›®æ ‡å˜é‡]
                                        **ç»Ÿè®¡ç»“æœ:**
                                        - å…³ç³»ç±»å‹: [åˆ†ç±»->æ•°å€¼/æ•°å€¼->æ•°å€¼/åˆ†ç±»->åˆ†ç±»]
                                        - ç»Ÿè®¡é‡: [Få€¼/ç›¸å…³ç³»æ•°/å¡æ–¹å€¼] = X
                                        - på€¼: X
                                        - æ˜¾è‘—æ€§: [é«˜åº¦æ˜¾è‘—/æ˜¾è‘—/ä¸æ˜¾è‘—]

                                        **ç±»åˆ«ç»Ÿè®¡:** (é€‚ç”¨äºåˆ†ç±»->æ•°å€¼å…³ç³»)
                                        | ç±»åˆ« | æ ·æœ¬æ•° | å‡å€¼ | æ ‡å‡†å·® | ä¸æ€»ä½“å‡å€¼å·®å¼‚ |
                                        | --- | --- | --- | --- | --- |

                                        **æ˜¾è‘—å·®å¼‚ç±»åˆ«:**
                                        - ç±»åˆ«X: å‡å€¼Y (æ¯”æ€»ä½“å‡å€¼[é«˜/ä½]Z%)

                                        # æŠ€æœ¯é™„å½•
                                        ## DAGåˆ†æå®Œæ•´æŠ¥å‘Š
                                        [æ­¤å¤„ç›´æ¥æ’å…¥åŸå§‹DAGæŠ¥å‘Šï¼Œä¸åšä¿®æ”¹]

                                        è¯·ç¡®ä¿ï¼š
                                        1. è¡¨æ ¼æ ¼å¼æ¸…æ™°ï¼Œæ•°æ®å¯¹é½
                                        2. æ‰€æœ‰ç»Ÿè®¡ç»“æœä¿ç•™é€‚å½“å°æ•°ä½æ•°(é€šå¸¸2-4ä½)
                                        3. å¯¹ç»Ÿè®¡æ˜¾è‘—æ€§ä½¿ç”¨æ ‡å‡†è¡¨ç¤º: *** p<0.001, ** p<0.01, * p<0.05, ns pâ‰¥0.05
                                        4. å¯¹éæŠ€æœ¯è¯»è€…è§£é‡Šç»Ÿè®¡æœ¯è¯­ï¼Œä½†ä¿æŒä¸“ä¸šæ€§
                                        5. é‡ç‚¹çªå‡ºå¼‚å¸¸å€¼ã€æ˜¾è‘—å·®å¼‚å’Œæœ‰å•†ä¸šä»·å€¼çš„å‘ç°
                                        6. æ‰€æœ‰ç»“è®ºå¿…é¡»æœ‰æ•°æ®æ”¯æŒï¼Œé¿å…è¿‡åº¦è§£è¯»
                                        7. å¯¹äºå¤æ‚å…³ç³»ï¼Œæä¾›ç®€æ˜çš„è§£é‡Šå’Œå¯èƒ½çš„å› æœæœºåˆ¶
                                        8. è¯·æä¾›å°½å¯èƒ½è¯¦ç»†çš„æ•°æ®æ´å¯Ÿå’Œä¸šåŠ¡å½±å“åˆ†æ"""
                                },
                                {
                                    "role": "user",
                                    "content": f"""
                                    åˆå§‹åˆ†æç»“æœï¼š{st.session_state.analysis_response}
                                    DAGåˆ†ææŠ¥å‘Šï¼š{dag_report}
                                    æ•°æ®æè¿°ä¿¡æ¯ï¼š{json_output}
                                    
                                    è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ•°æ®åˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šåº”ä¸¥æ ¼éµå¾ªç³»ç»Ÿæç¤ºä¸­çš„ç»“æ„å’Œæ ¼å¼è¦æ±‚ï¼Œç‰¹åˆ«æ³¨æ„ï¼š
                                    1. å……åˆ†åˆ©ç”¨DAGåˆ†ææŠ¥å‘Šä¸­çš„ç»Ÿè®¡ç»“æœï¼ŒåŒ…æ‹¬Få€¼ã€på€¼å’Œç±»åˆ«ç»Ÿè®¡
                                    2. æ•´åˆæ•°æ®æè¿°ä¸­çš„ç»Ÿè®¡ä¿¡æ¯å’Œåˆ†å¸ƒç‰¹å¾
                                    3. æ‰€æœ‰è¡¨æ ¼å¿…é¡»æ ¼å¼è§„èŒƒï¼Œæ•°æ®å¯¹é½
                                    4. é‡ç‚¹å…³æ³¨ç»Ÿè®¡æ˜¾è‘—çš„å…³ç³»å’Œå¼‚å¸¸å€¼
                                    5. ç¡®ä¿éæŠ€æœ¯äººå‘˜ä¹Ÿèƒ½ç†è§£æŠ¥å‘Šå†…å®¹
                                    6. æ‰€æœ‰ç»“è®ºå’Œå»ºè®®å¿…é¡»æœ‰æ•°æ®æ”¯æŒ
                                    """
                                }
                            ],
                            temperature=0.7,
                            max_tokens=5000,
                            stream=True
                        )
                        
                        full_response = ""
                        reasoning_content = ""
                        
                        # å¤„ç†æµå¼å“åº”
                        for chunk in response:
                            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                                reasoning_content += chunk.choices[0].delta.reasoning_content
                                dag_progress.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nåˆ†æç»“æœï¼š\n{full_response}")
                            elif hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                                full_response += chunk.choices[0].delta.content
                                dag_progress.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nåˆ†æç»“æœï¼š\n{full_response}")
                        
                        # ä¿å­˜åˆ°session state
                         # --- æ–°å¢çš„æ¸…ç†æ­¥éª¤ ---
                        cleaned_report = full_response.strip()
                        # æ£€æŸ¥å¹¶ç§»é™¤åŒ…è£¹çš„ä»£ç å—æ ‡è®°
                        if cleaned_report.startswith("```markdown"):
                            cleaned_report = cleaned_report[len("```markdown"):].strip()
                        if cleaned_report.startswith("```"):
                            cleaned_report = cleaned_report[3:].strip()
                        if cleaned_report.endswith("```"):
                            cleaned_report = cleaned_report[:-3].strip()

                        # ä¿å­˜æ¸…ç†åçš„æŠ¥å‘Šåˆ°session state
                        st.session_state.business_report = cleaned_report  # <-- ä½¿ç”¨æ¸…ç†åçš„å˜é‡
                
                        # st.session_state.business_report = full_response
                        st.session_state.dag_report = dag_report  # ç¡®ä¿è¿™è¡Œå­˜åœ¨ï¼Œä½ çš„ä»£ç é‡Œå·²ç»æœ‰äº†
                        st.session_state.data_description = json_output # ç¡®ä¿è¿™è¡Œå­˜åœ¨ï¼Œä½ çš„ä»£ç é‡Œå·²ç»æœ‰äº†
                        st.session_state.dag_reasoning = reasoning_content
                        
                        # æ¸…ç©ºè¿›åº¦å®¹å™¨
                        dag_progress.empty()
                        
                except Exception as e:
                    st.error(f"DAGåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        
        # ä½¿ç”¨ä¸¤åˆ—æ˜¾ç¤ºåˆ†æç»“æœå’ŒDAGåˆ†æç»“æœ
        if st.session_state.analysis_response or st.session_state.business_report:
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### æ•°æ®åˆ†æ")
                if st.session_state.analysis_reasoning:
                    with st.expander("æ€è€ƒè¿‡ç¨‹", expanded=False):
                        st.markdown(st.session_state.analysis_reasoning)
                st.markdown(st.session_state.analysis_response)
                
                # DAGå®šä¹‰ç¼–è¾‘å™¨
                st.markdown("### DAGå®šä¹‰")
                dag_editor = st.text_area("ç¼–è¾‘DAGè¾¹å®šä¹‰:", value=st.session_state.dag_edges, height=200)
                if dag_editor != st.session_state.dag_edges:
                    st.session_state.dag_edges = dag_editor
                # Generate the Mermaid HTML
                try:
                    # ç¡®ä¿å…ˆè§£ææ–‡æœ¬ä¸­çš„dag_edges
                    dag_edges = extract_dag_edges(st.session_state.dag_edges)
                    if dag_edges:
                        mermaid_html = create_mermaid_html_from_edges(dag_edges)
                        # æ¸²æŸ“ Mermaid å›¾ï¼Œå¹¶å¢åŠ é«˜åº¦ä»¥é€‚åº”ç¼©æ”¾
                        st.markdown("## å…³ç³»å›¾ (ä½¿ç”¨å³ä¸Šè§’æŒ‰é’®ç¼©æ”¾)")
                        st.components.v1.html(mermaid_html, height=600, scrolling=True)
                    else:
                        st.warning("æ— æ³•è§£æDAGè¾¹å®šä¹‰ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
                except Exception as e:
                    st.error(f"ç”Ÿæˆå…³ç³»å›¾æ—¶å‡ºé”™: {str(e)}")
            
            with col2:
                if st.session_state.business_report:
                    st.markdown("### DAGåˆ†æç»“æœ")

                    # æ˜¾ç¤ºLLMçš„åˆ†æè¿‡ç¨‹
                    if hasattr(st.session_state, 'dag_reasoning') and st.session_state.dag_reasoning:
                        with st.expander("LLMåˆ†æè¿‡ç¨‹", expanded=False):
                            st.markdown(st.session_state.dag_reasoning)
                    
                    # 1. ä½¿ç”¨ st.markdown() æ˜¾ç¤ºæ ¼å¼åŒ–çš„å•†ä¸šæŠ¥å‘Š
                    st.markdown(st.session_state.business_report)

                    # 2. ä½¿ç”¨ st.expander + st.text() æ˜¾ç¤ºåŸå§‹çš„DAGæŠ€æœ¯æŠ¥å‘Š
                    if hasattr(st.session_state, 'dag_report') and st.session_state.dag_report:
                        with st.expander("æŸ¥çœ‹DAGåˆ†æåŸå§‹æŠ¥å‘Š (Technical Appendix)", expanded=False):
                            # st.text() èƒ½å¾ˆå¥½åœ°ä¿ç•™åŸå§‹æ–‡æœ¬çš„å¯¹é½å’Œæ¢è¡Œ
                            st.text(st.session_state.dag_report)

                    # 3. ä½¿ç”¨ st.expander + st.json() æ˜¾ç¤ºæè¿°æ€§ç»Ÿè®¡çš„JSONæ•°æ®
                    if hasattr(st.session_state, 'data_description') and st.session_state.data_description:
                        with st.expander("æŸ¥çœ‹æè¿°æ€§ç»Ÿè®¡JSONæ•°æ®", expanded=False):
                            # st.json() ä¼šç¾åŒ–æ˜¾ç¤ºJSON
                            st.json(st.session_state.data_description)
            
            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            if st.session_state.business_report:
                st.markdown("---")
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½åˆ†ææŠ¥å‘Š",
                    data=create_word_document(st.session_state.business_report),
                    file_name=f"data_analysis_report_{time.strftime('%Y%m%d_%H%M%S')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )

# Add this function before the main() function
def setup_sales_forecasting():
    st.markdown(
        """
    <h1 style='text-align: center; font-size: 18px; font-weight: bold;'>Sales Forecasting Application</h1>
    <h6 style='text-align: center; font-size: 12px;'>Upload your Excel file to forecast sales or other time series data</h6>
    <br><br><br>
    """,
        unsafe_allow_html=True,
    )

    # Set random seed for reproducibility
    np.random.seed(42)
        
    # Initialize session state for storing results between interactions
    if 'forecast_df' not in st.session_state:
        st.session_state.forecast_df = None
    if 'all_groups' not in st.session_state:
        st.session_state.all_groups = []
    if 'has_forecast' not in st.session_state:
        st.session_state.has_forecast = False
    if 'target_column' not in st.session_state:
        st.session_state.target_column = None
    if 'original_filename' not in st.session_state:
        st.session_state.original_filename = None
    if 'training_accuracy' not in st.session_state:
        st.session_state.training_accuracy = {}
    if 'validation_accuracy' not in st.session_state:
        st.session_state.validation_accuracy = {}
    if 'start_date_str' not in st.session_state:
        st.session_state.start_date_str = None
    if 'training_end_date_str' not in st.session_state:
        st.session_state.training_end_date_str = None
    if 'covariate_columns' not in st.session_state:
        st.session_state.covariate_columns = []
    
    st.markdown("""
    This app helps you forecast sales or other time series data using Facebook Prophet.
    Upload your Excel file, configure the settings, and get predictions for future periods.
    """)
    
    # File uploader
    uploaded_file = st.file_uploader("Upload Excel file", type=["xlsx", "xls"])
    
    if uploaded_file is not None:
        # Store original filename
        if st.session_state.original_filename is None:
            st.session_state.original_filename = uploaded_file.name.split('.')[0]
            
        # Load the file and display sheet selection
        excel_file = pd.ExcelFile(uploaded_file)
        sheet_name = st.selectbox("Select Sheet", excel_file.sheet_names)
        
        # Read the selected sheet
        df = pd.read_excel(uploaded_file, sheet_name=sheet_name)
        
        # Display data preview
        st.subheader("Data Preview")
        st.dataframe(df.head())
        
        # Basic data info
        st.subheader("Dataset Information")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"Number of rows: {df.shape[0]}")
        with col2:
            st.write(f"Number of columns: {df.shape[1]}")
        
        # Column selection and configuration
        st.subheader("Configure Forecasting Parameters")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Date column selection
            date_column_options = df.columns.tolist()
            date_column = st.selectbox("Select Date/Month Column", date_column_options)
            
            # Date format selection
            date_format_options = [
                "%Y%m", "%Y-%m", "%b %Y", "%B %Y", 
                "%Y%m%d", "%Y-%m-%d", "%d-%m-%Y", "%m/%d/%Y"
            ]
            date_format = st.selectbox("Select Date Format", date_format_options)
            
            # ç¡®å®šé¢„æµ‹é¢‘ç‡ - æ ¹æ®æ—¥æœŸæ ¼å¼è‡ªåŠ¨è°ƒæ•´
            forecast_freq = 'D' if any(x in date_format for x in ['%d', '%Y%m%d', '%Y-%m-%d', '%d-%m-%Y', '%m/%d/%Y']) else 'MS'
            
            # Target column (to be predicted)
            numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
            target_column = st.selectbox("Select Target Column to Forecast", numeric_columns)
            st.session_state.target_column = target_column
    
            # Start date for training
            min_date = pd.to_datetime("2020-01-01")  # Default minimum date
            max_date = datetime.now()
            start_date = st.date_input("Training Start Date", 
                                      value=pd.to_datetime("2022-01-01"),
                                      min_value=min_date,
                                      max_value=max_date)
            
            # Covariate selection - NEW FEATURE
            # Filter out date column and target column from potential covariates
            potential_covariates = [col for col in numeric_columns if col != target_column]
            covariate_columns = st.multiselect("Select Covariate Columns (Optional)", potential_covariates, 
                                               help="Additional numeric variables to improve forecast accuracy")
            st.session_state.covariate_columns = covariate_columns
            
            # æ·»åŠ æ¨¡å‹é€‰æ‹©
            use_advanced_model = st.checkbox("ä½¿ç”¨é«˜çº§æ¨¡å‹ (æ›´é«˜ç²¾åº¦ä½†æ›´æ…¢)", value=False)   
        
        with col2:
            # Grouping columns (multi-select)
            all_columns = df.columns.tolist()
            grouping_columns = st.multiselect("Select Columns for Grouping (Optional)", all_columns)
    
            # Forecast horizon
            forecast_years = st.number_input("Forecast Horizon (Years)", min_value=1, max_value=10, value=3)
            
            # Add a forecast end date input
            current_date = datetime.now()
            default_end_date = current_date.replace(day=1) + pd.DateOffset(years=forecast_years) - pd.DateOffset(days=1)
            end_date = st.date_input("Forecast End Date", 
                                    value=default_end_date,
                                    min_value=current_date)
            
            # Hidden Confidence Interval Width (not displayed in UI)
            interval_width = 0.6  # Set default to 0.6 as requested
            
            # Add a training end date input at the bottom of col2
            training_end_date = st.date_input("Training End Date", 
                                    value=current_date,
                                    min_value=start_date,
                                    max_value=current_date)
        
        # Run forecast button
        if st.button("Run Forecast") or st.session_state.has_forecast:
            if not st.session_state.has_forecast:
                try:
                    # Progress bar
                    progress_bar = st.progress(0)
                    
                    # Copy the dataframe to avoid modifying the original
                    df_copy = df.copy()
                    
                    # Process date column
                    st.info("Processing date column...")
                    try:
                        df_copy['date'] = pd.to_datetime(df_copy[date_column].astype(str), format=date_format)
                    except:
                        df_copy['date'] = pd.to_datetime(df_copy[date_column].astype(str))
                        
                    # Store the original date format from the column
                    original_date_values = df_copy[date_column].copy()
                        
                    progress_bar.progress(10)
                    
                    # Create grouping key if group columns are selected
                    if grouping_columns:
                        st.info("Creating group combinations...")
                        df_copy['group'] = df_copy[grouping_columns[0]].astype(str)
                        for col in grouping_columns[1:]:
                            df_copy['group'] += '_' + df_copy[col].astype(str)
                    else:
                        # If no grouping is selected, create a single group
                        df_copy['group'] = 'all_data'
                        
                    progress_bar.progress(20)
                    
                    # Get current date and calculate end date for forecasting
                    current_date = datetime.now()
                    end_date_str = end_date.strftime('%Y-%m-%d')
                    training_end_date_str = training_end_date.strftime('%Y-%m-%d')
                    start_date_str = start_date.strftime('%Y-%m-%d')
                    
                    # Store date strings in session state for later use
                    st.session_state.start_date_str = start_date_str
                    st.session_state.training_end_date_str = training_end_date_str
                    
                    # Create complete date and group combinations
                    # åˆ›å»ºå®Œæ•´æ—¥æœŸå’Œåˆ†ç»„ç»„åˆï¼Œä½¿ç”¨åŠ¨æ€é¢‘ç‡
                    st.info("Creating complete date-group combinations...")
                    # ç¡®ä¿ç”Ÿæˆçš„æ—¥æœŸèŒƒå›´å§‹ç»ˆå»¶ä¼¸åˆ°é¢„æµ‹ç»“æŸæ—¥æœŸ
                    all_dates = pd.date_range(start=df_copy['date'].min(), end=end_date_str, freq=forecast_freq)
                    all_groups = df_copy['group'].unique()
                    st.session_state.all_groups = all_groups.tolist()
                    
                    complete_df = pd.DataFrame([(date, group) for date in all_dates for group in all_groups],
                                              columns=['date', 'group'])
                    
                    # Merge with original data
                    df_copy = pd.merge(complete_df, df_copy, on=['date', 'group'], how='left')
                    
                    # Fill missing values for target column with 0
                    df_copy[target_column] = df_copy[target_column].fillna(0)
                    
                    # Handle covariates - calculate means for each covariate within its group
                    covariate_means = {}
                    if covariate_columns:
                        for col in covariate_columns:
                            # Calculate mean for each group
                            for group in all_groups:
                                group_data = df_copy[(df_copy['group'] == group) & df_copy[col].notna()]
                                if not group_data.empty:
                                    # Use the mean of non-NA values for this group
                                    covariate_means[(group, col)] = group_data[col].mean()
                                else:
                                    # If all values are NA for this group, use global mean
                                    global_mean = df_copy[df_copy[col].notna()][col].mean()
                                    covariate_means[(group, col)] = global_mean if not np.isnan(global_mean) else 0
                            
                            # Fill NAs with group-specific means
                            for group in all_groups:
                                mask = (df_copy['group'] == group) & df_copy[col].isna()
                                df_copy.loc[mask, col] = covariate_means.get((group, col), 0)
                    
                    # Forward fill other columns within groups
                    if grouping_columns:
                        for col in grouping_columns:
                            df_copy[col] = df_copy.groupby('group')[col].ffill().bfill()
                            
                    progress_bar.progress(40)
                    
                    # Sort data
                    df_copy = df_copy.sort_values(by=['group', 'date'], ascending=[True, True])
                    
                    # Convert dates to string format
                    current_date_str = current_date.strftime("%Y-%m-%d")
                    
                    # Run Prophet forecast for each group
                    st.info("Running forecasts for each group...")
                    forecasts = {}
                    training_accuracy = {}
                    validation_accuracy = {}
                    total_groups = len(all_groups)
                    
                    for i, group in enumerate(all_groups):
                        # Update progress based on group progress
                        progress_value = 40 + (i / total_groups * 50)
                        progress_bar.progress(int(progress_value))
                        
                        # Filter data for this group - using the start_date parameter
                        group_data = df_copy[(df_copy['group'] == group) & 
                                           (df_copy['date'] >= start_date_str) & 
                                           (df_copy['date'] <= training_end_date_str)].copy()
                        
                        group_data = group_data.rename(columns={'date': 'ds', target_column: 'y'})
                        
                        # Add covariates to Prophet model if selected
                        if use_advanced_model:
                            model = Prophet(interval_width=interval_width, uncertainty_samples=1000, mcmc_samples=300)
                        else:
                            model = Prophet(interval_width=interval_width)
                            
                        # Add covariates to model
                        if covariate_columns:
                            for col in covariate_columns:
                                model.add_regressor(col)
    
                        try:
                            model.fit(group_data[['ds', 'y'] + covariate_columns])
                            
                            # ä¿®æ”¹: ç›´æ¥ä½¿ç”¨é¢„æµ‹ç»“æŸæ—¥æœŸæ¥åˆ›å»ºfuture dataframe
                            future_end_date = pd.to_datetime(end_date_str)
                            # åˆ›å»ºä»è®­ç»ƒæ•°æ®å¼€å§‹åˆ°é¢„æµ‹ç»“æŸæ—¥æœŸçš„å®Œæ•´æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨åŠ¨æ€é¢‘ç‡
                            future_dates = pd.date_range(start=group_data['ds'].min(), end=future_end_date, freq=forecast_freq)
                            future = pd.DataFrame({'ds': future_dates})
                            
                            # Add covariate values to future dataframe for prediction
                            if covariate_columns:
                                for col in covariate_columns:
                                    # Get all values for this group and covariate
                                    historical_values = df_copy[(df_copy['group'] == group) & 
                                                              (df_copy['date'].isin(future_dates))][col].values
                                    
                                    # Create DataFrame with future dates and corresponding covariate values
                                    covariate_df = pd.DataFrame({
                                        'ds': future_dates[:len(historical_values)],
                                        col: historical_values
                                    })
                                    
                                    # For dates beyond historical data, use the group mean
                                    if len(future_dates) > len(historical_values):
                                        remaining_dates = future_dates[len(historical_values):]
                                        remaining_values = np.full(len(remaining_dates), 
                                                                  covariate_means.get((group, col), 0))
                                        
                                        remaining_df = pd.DataFrame({
                                            'ds': remaining_dates,
                                            col: remaining_values
                                        })
                                        
                                        covariate_df = pd.concat([covariate_df, remaining_df])
                                    
                                    # Merge with future DataFrame
                                    future = pd.merge(future, covariate_df, on='ds', how='left')
                            
                            # Make forecast
                            forecast = model.predict(future)
                            
                            forecasts[group] = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
                        except Exception as e:
                            st.warning(f"Could not forecast for group {group}: {str(e)}")
                            # Create empty forecast data for this group
                            forecasts[group] = pd.DataFrame({
                                'ds': all_dates,
                                'yhat': np.zeros(len(all_dates)),
                                'yhat_lower': np.zeros(len(all_dates)),
                                'yhat_upper': np.zeros(len(all_dates))
                            })
                    
                    progress_bar.progress(90)
                    
                    # Write forecasts back to the dataframe using direct array assignment
                    st.info("Combining results...")
                    for group, forecast in forecasts.items():
                        # Use date matching approach instead of direct array assignment
                        for i, row in forecast.iterrows():
                            forecast_date = row['ds']
                            # Match both group and date
                            mask = (df_copy['group'] == group) & (df_copy['date'] == forecast_date)
                            # Assign forecast values
                            df_copy.loc[mask, 'forecast'] = row['yhat']
                            df_copy.loc[mask, 'forecast_lower'] = row['yhat_lower']
                            df_copy.loc[mask, 'forecast_upper'] = row['yhat_upper']
                    
                    # Format dates in the original date column according to the selected format
                    # This preserves the original date format while extending to future dates
                    df_copy[date_column] = df_copy['date'].dt.strftime(date_format)
                    
                    # Calculate prediction accuracy percentages for each group
                    for group in all_groups:
                        # Training period accuracy (Training Start Date to Training End Date)
                        training_data = df_copy[(df_copy['group'] == group) & 
                                              (df_copy['date'] >= start_date_str) & 
                                              (df_copy['date'] <= training_end_date_str)]
                        
                        if not training_data.empty and training_data[target_column].sum() > 0:
                            actual = training_data[target_column].values
                            pred = training_data['forecast'].values
                            # Calculate percentage error: (pred - actual) / actual
                            pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                            # Calculate MAPE (Mean Absolute Percentage Error)
                            mean_pct_error = np.nanmean(pct_errors) * 100
                            training_accuracy[group] = mean_pct_error
                        else:
                            training_accuracy[group] = np.nan
                        
                        # Validation period accuracy (Training End Date to current date)
                        validation_data = df_copy[(df_copy['group'] == group) & 
                                                (df_copy['date'] > training_end_date_str) & 
                                                (df_copy['date'] <= current_date_str)]
                        
                        if not validation_data.empty and validation_data[target_column].sum() > 0:
                            actual = validation_data[target_column].values
                            pred = validation_data['forecast'].values
                            # Calculate percentage error: (pred - actual) / actual
                            pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                            # Calculate MAPE
                            mean_pct_error = np.nanmean(pct_errors) * 100
                            validation_accuracy[group] = mean_pct_error
                        else:
                            validation_accuracy[group] = np.nan
                    
                    # Store accuracy metrics in session state
                    st.session_state.training_accuracy = training_accuracy
                    st.session_state.validation_accuracy = validation_accuracy
                    
                    # Store results in session state
                    st.session_state.forecast_df = df_copy
                    st.session_state.has_forecast = True
                    
                    progress_bar.progress(100)
                    st.success("Forecast completed successfully!")
                    
                except Exception as e:
                    st.error(f"An error occurred during forecasting: {str(e)}")
                    st.error("Please check your data and settings and try again.")
            
            # Display results if available in session state
            if st.session_state.has_forecast and st.session_state.forecast_df is not None:
                # Display results
                st.subheader("Forecast Results")
                st.dataframe(st.session_state.forecast_df)
                
                # Create download link for the forecast
                st.subheader("Download Complete Forecast")
                
                # Get the original filename for downloads
                filename_base = st.session_state.original_filename
                
                # CSV download with custom filename
                csv = st.session_state.forecast_df.to_csv(index=False)
                b64 = base64.b64encode(csv.encode()).decode()
                csv_filename = f"{filename_base}_forecast.csv"
                href = f'<a href="data:file/csv;base64,{b64}" download="{csv_filename}">Download CSV File</a>'
                st.markdown(href, unsafe_allow_html=True)
                
                # Excel download with custom filename
                buffer = io.BytesIO()
                st.session_state.forecast_df.to_excel(buffer, index=False)
                buffer.seek(0)
                b64_excel = base64.b64encode(buffer.read()).decode()
                excel_filename = f"{filename_base}_forecast.xlsx"
                href_excel = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64_excel}" download="{excel_filename}">Download Excel File</a>'
                st.markdown(href_excel, unsafe_allow_html=True)
                
                # Visualization section
                st.subheader("Forecast Visualizations")
                
                # Create dropdown for group selection
                selected_group = st.selectbox("Select group to visualize", st.session_state.all_groups)
                
                # Get accuracy metrics for selected group
                training_acc = st.session_state.training_accuracy.get(selected_group, np.nan)
                validation_acc = st.session_state.validation_accuracy.get(selected_group, np.nan)
                
                # Format accuracy metrics for display
                training_acc_text = f"Training Accuracy: {training_acc:.2f}%" if not np.isnan(training_acc) else "Training Accuracy: N/A"
                validation_acc_text = f"Validation Accuracy: {validation_acc:.2f}%" if not np.isnan(validation_acc) else "Validation Accuracy: N/A"
                
                # Plot the selected group
                selected_data = st.session_state.forecast_df[st.session_state.forecast_df['group'] == selected_group].copy()
                
                # ä¿®æ”¹: ç¡®ä¿ä½¿ç”¨æ’åºåçš„æ•°æ®æ¥åˆ›å»ºå›¾è¡¨ï¼Œé¿å…confidence intervalçš„æ˜¾ç¤ºé—®é¢˜
                selected_data = selected_data.sort_values('date')
                
                # Create plotly figure for selected group
                fig = make_subplots(specs=[[{"secondary_y": True}]])
                
                # Add actual values
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data[st.session_state.target_column],
                        mode='lines+markers',
                        name='Actual',
                        line=dict(color='blue')
                    )
                )
                
                # Add forecast
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data['forecast'],
                        mode='lines',
                        name='Forecast',
                        line=dict(color='red')
                    )
                )
                
                # ä¿®æ”¹: æ›´æ”¹confidence intervalçš„ç»˜åˆ¶æ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®ç»˜åˆ¶
                # æ·»åŠ ä¸Šè¾¹ç•Œ
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data['forecast_upper'],
                        mode='lines',
                        line=dict(width=0),
                        showlegend=False
                    )
                )
                
                # æ·»åŠ ä¸‹è¾¹ç•Œ
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data['forecast_lower'],
                        mode='lines',
                        line=dict(width=0),
                        fillcolor='rgba(255,0,0,0.2)',
                        fill='tonexty',
                        name='Confidence Interval'
                    )
                )
                
                # Update layout with accuracy metrics in the title
                fig.update_layout(
                    title=f'Forecast for {selected_group} ({training_acc_text}, {validation_acc_text})',
                    xaxis_title='Date',
                    yaxis_title=st.session_state.target_column,
                    hovermode='x unified',
                    legend=dict(orientation='h', y=1.1)
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Add option to view overall forecast (sum of all groups)
                if st.checkbox("View Overall Forecast (Sum of All Groups)"):
                    # Group by date and sum the values
                    overall_data = st.session_state.forecast_df.groupby('date').agg({
                        st.session_state.target_column: 'sum',
                        'forecast': 'sum',
                        'forecast_lower': 'sum',
                        'forecast_upper': 'sum'
                    }).reset_index()
                    
                    # ä¿®æ”¹: ç¡®ä¿ä½¿ç”¨æ’åºåçš„æ•°æ®æ¥åˆ›å»ºå›¾è¡¨ï¼Œé¿å…confidence intervalçš„æ˜¾ç¤ºé—®é¢˜
                    overall_data = overall_data.sort_values('date')
                    
                    # Calculate overall accuracy metrics
                    # Convert dates to datetime for comparison
                    overall_data['date_dt'] = pd.to_datetime(overall_data['date'])
                    
                    # Use stored date strings from session state
                    start_date_str = st.session_state.start_date_str
                    training_end_date_str = st.session_state.training_end_date_str
                    
                    # Convert to datetime objects for comparison
                    start_date_dt = pd.to_datetime(start_date_str)
                    training_end_date_dt = pd.to_datetime(training_end_date_str)
                    current_date_dt = pd.to_datetime(current_date.strftime("%Y-%m-%d"))
                    
                    # Training period accuracy (Training Start Date to Training End Date)
                    training_overall = overall_data[(overall_data['date_dt'] >= start_date_dt) & 
                                                  (overall_data['date_dt'] <= training_end_date_dt)]
                    
                    if not training_overall.empty and training_overall[st.session_state.target_column].sum() > 0:
                        actual = training_overall[st.session_state.target_column].values
                        pred = training_overall['forecast'].values
                        # Calculate percentage error: (pred - actual) / actual
                        pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                        # Calculate MAPE
                        overall_training_acc = np.nanmean(pct_errors) * 100
                    else:
                        overall_training_acc = np.nan
                    
                    # Validation period accuracy (Training End Date to current date)
                    validation_overall = overall_data[(overall_data['date_dt'] > training_end_date_dt) & 
                                                    (overall_data['date_dt'] <= current_date_dt)]
                    
                    if not validation_overall.empty and validation_overall[st.session_state.target_column].sum() > 0:
                        actual = validation_overall[st.session_state.target_column].values
                        pred = validation_overall['forecast'].values
                        # Calculate percentage error: (pred - actual) / actual
                        pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                        # Calculate MAPE
                        overall_validation_acc = np.nanmean(pct_errors) * 100
                    else:
                        overall_validation_acc = np.nan
                    
                    # Format overall accuracy metrics for display
                    overall_training_acc_text = f"Training Accuracy: {overall_training_acc:.2f}%" if not np.isnan(overall_training_acc) else "Training Accuracy: N/A"
                    overall_validation_acc_text = f"Validation Accuracy: {overall_validation_acc:.2f}%" if not np.isnan(overall_validation_acc) else "Validation Accuracy: N/A"
                    
                    # Create plotly figure for overall data
                    fig_overall = make_subplots(specs=[[{"secondary_y": True}]])
                    
                    # Add actual values
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data[st.session_state.target_column],
                            mode='lines+markers',
                            name='Actual',
                            line=dict(color='blue')
                        )
                    )
                    
                    # Add forecast
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data['forecast'],
                            mode='lines',
                            name='Forecast',
                            line=dict(color='red')
                        )
                    )
                    
                    # ä¿®æ”¹: æ›´æ”¹confidence intervalçš„ç»˜åˆ¶æ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®ç»˜åˆ¶
                    # æ·»åŠ ä¸Šè¾¹ç•Œ
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data['forecast_upper'],
                            mode='lines',
                            line=dict(width=0),
                            showlegend=False
                        )
                    )
                    
                    # æ·»åŠ ä¸‹è¾¹ç•Œ
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data['forecast_lower'],
                            mode='lines',
                            line=dict(width=0),
                            fillcolor='rgba(255,0,0,0.2)',
                            fill='tonexty',
                            name='Confidence Interval'
                        )
                    )
                    
                    # Update layout with accuracy metrics in the title
                    fig_overall.update_layout(
                        title=f'Overall Forecast (Sum of All Groups) ({overall_training_acc_text}, {overall_validation_acc_text})',
                        xaxis_title='Date',
                        yaxis_title=st.session_state.target_column,
                        hovermode='x unified',
                        legend=dict(orientation='h', y=1.1)
                    )
                    
                    st.plotly_chart(fig_overall, use_container_width=True)
        
        # Add button to clear results and start over
        if st.session_state.has_forecast:
            if st.button("Clear Results and Start Over"):
                # Fix for experimental_rerun issue - use session state to clear data
                st.session_state.forecast_df = None
                st.session_state.all_groups = []
                st.session_state.has_forecast = False
                st.session_state.target_column = None
                st.session_state.original_filename = None
                st.session_state.training_accuracy = {}
                st.session_state.validation_accuracy = {}
                st.session_state.start_date_str = None
                st.session_state.training_end_date_str = None
                # Use Streamlit's rerun function instead of experimental_rerun
                st.rerun()
    
    else:
        # Display sample instructions when no file is uploaded
        st.info("Please upload an Excel file to begin. Your file should contain:")
        st.markdown("""
        - A column with dates or year-month values
        - At least one numeric column to forecast
        - Optional grouping columns (e.g., product codes, regions)
        
        The app will:
        1. Process your data
        2. Fill missing values
        3. Generate forecasts using Facebook Prophet
        4. Allow you to download the complete forecast results
        """)
        
        # Sample data structure
        st.subheader("Sample Data Structure")
        sample_data = pd.DataFrame({
            'year_month': ['202201', '202202', '202203', '202201', '202202', '202203'],
            'product_code': ['A001', 'A001', 'A001', 'B002', 'B002', 'B002'],
            'region': ['North', 'North', 'North', 'South', 'South', 'South'],
            'sales_quantity': [100, 120, 110, 80, 85, 95],
            'sales_amount': [5000, 6000, 5500, 4000, 4250, 4750]
        })
        
        st.dataframe(sample_data)
    
    # Footer
    st.markdown("""
    ---
    ### Notes:
    - For best results, provide at least 12 months of historical data
    - If using grouping, ensure all groups have sufficient data points
    - The forecast horizon can be adjusted based on your needs
    """)

# åœ¨æ–‡ä»¶é¡¶éƒ¨çš„å¯¼å…¥éƒ¨åˆ†ä¸‹æ·»åŠ è¿™ä¸ªè¾…åŠ©å‡½æ•°
def convert_cluster_label(label, naming_style):
    """å°†æ•°å­—æ ‡ç­¾è½¬æ¢ä¸ºé€‰å®šçš„å‘½åé£æ ¼"""
    label_num = int(label)
    if naming_style == "æ•°å­—":
        return str(label_num)
    elif naming_style == "å­—æ¯":
        # å°† 0, 1, 2, ... è½¬æ¢ä¸º A, B, C, ...
        return chr(65 + label_num) if label_num < 26 else f"A{label_num-25}"
    elif naming_style == "ä¸­æ–‡":
        # å°† 0, 1, 2, ... è½¬æ¢ä¸º ç”², ä¹™, ä¸™, ...
        chinese_labels = ['ç”²', 'ä¹™', 'ä¸™', 'ä¸', 'æˆŠ', 'å·±', 'åºš', 'è¾›', 'å£¬', 'ç™¸']
        if label_num < len(chinese_labels):
            return chinese_labels[label_num]
        else:
            # è¶…å‡ºåŸºæœ¬çš„10ä¸ªåä½¿ç”¨"ç”²1, ç”²2, ..."æ ¼å¼
            return f"{chinese_labels[0]}{label_num-9}"
    return str(label_num)  # é»˜è®¤è¿”å›æ•°å­—

def setup_clustering_analysis():
    st.markdown(
        """
    <h1 style='text-align: center; font-size: 18px; font-weight: bold;'>K-means/DBSCANèšç±»ä¸çƒ­åŠ›å›¾åˆ†æå·¥å…·</h1>
    <h6 style='text-align: center; font-size: 12px;'>ä¸Šä¼ CSVæˆ–Excelæ–‡ä»¶è¿›è¡ŒK-means/DBSCANèšç±»ä¸çƒ­åŠ›å›¾åˆ†æ</h6>
    <br><br><br>
    """,
        unsafe_allow_html=True,
    )

    # åº”ç”¨æ ‡é¢˜
    st.title("K-means/DBSCANèšç±»ä¸çƒ­åŠ›å›¾åˆ†æå·¥å…·")

    # è¾¹æ è¯´æ˜
    with st.sidebar:
        st.header("ä½¿ç”¨è¯´æ˜")
        st.write("""
        1. ä¸Šä¼  CSV æˆ– Excel æ•°æ®æ–‡ä»¶
        2. é€‰æ‹©èšç±»æ–¹å¼:
        - ä½¿ç”¨ K-means æˆ– DBSCAN è¿›è¡Œèšç±»
        - æˆ–ç›´æ¥ä½¿ç”¨å·²æœ‰çš„èšç±»åˆ—
        3. è§‚å¯Ÿçƒ­åŠ›å›¾åˆ†æç»“æœ
        4. ä¸‹è½½åˆ†æç»“æœ
        """)

    # ä¸Šä¼ æ–‡ä»¶
    uploaded_file = st.file_uploader("ä¸Šä¼ æ•°æ®æ–‡ä»¶", type=["csv", "xlsx", "xls"])

    if uploaded_file is not None:
        # è¯»å–æ•°æ®
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.success("æ–‡ä»¶ä¸Šä¼ æˆåŠŸ!")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.subheader("æ•°æ®é¢„è§ˆ")
            st.dataframe(df.head())
            
            # èšç±»æ¨¡å¼é€‰æ‹©
            clustering_mode = st.radio(
                "é€‰æ‹©èšç±»æ¨¡å¼:",
                ["ä½¿ç”¨æ¨¡å‹èšç±»", "ä½¿ç”¨å·²æœ‰èšç±»åˆ—"]
            )
            
            # ç”¨äºå­˜å‚¨ä¸¤ç§èšç±»ç»“æœçš„å˜é‡
            cluster_col1 = None
            cluster_col2 = None
            
            # ç¬¬ä¸€ä¸ªèšç±»
            st.subheader("ç¬¬ä¸€ä¸ªèšç±»")
            
            if clustering_mode == "ä½¿ç”¨æ¨¡å‹èšç±»":
                # é€‰æ‹©èšç±»ç®—æ³•
                clustering_algo = st.selectbox("é€‰æ‹©èšç±»ç®—æ³•", ["K-means", "DBSCAN"])
                
                # æ·»åŠ ç°‡å‘½åé£æ ¼é€‰æ‹©
                naming_style1 = st.selectbox(
                    "é€‰æ‹©ç°‡å‘½åé£æ ¼",
                    ["å­—æ¯", "æ•°å­—", "ä¸­æ–‡"],
                    index=0,  # é»˜è®¤é€‰æ‹©å­—æ¯
                    key="naming_style1"
                )
                # é€‰æ‹©ç”¨äºèšç±»çš„åˆ—
                numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
                if len(numeric_cols) > 0:
                    selected_cols1 = st.multiselect(
                        "é€‰æ‹©ç”¨äºç¬¬ä¸€ä¸ªèšç±»çš„åˆ—",
                        numeric_cols,
                        default=numeric_cols[:max(0,min(3, len(numeric_cols)))]
                    )
                    
                    if selected_cols1:
                        X1 = df[selected_cols1]
                        # æ ‡å‡†åŒ–æ•°æ®
                        scaler = StandardScaler()
                        X_scaled1 = scaler.fit_transform(X1)
                        
                        # å¤šç»´æ•°æ®é™ç»´é€‰é¡¹
                        if len(selected_cols1) >= 2:
                            input_option1 = st.radio(
                                "é€‰æ‹©èšç±»è¾“å…¥æ•°æ®ï¼ˆç¬¬ä¸€ä¸ªèšç±»ï¼‰",
                                ["åŸå§‹æ•°æ®", "t-SNEé™ç»´åˆ°2ç»´"],
                                index=0
                            )
                            if input_option1 == "t-SNEé™ç»´åˆ°2ç»´":
                                tsne = TSNE(n_components=2, random_state=42)
                                X_scaled1 = tsne.fit_transform(X_scaled1)
                        
                        if clustering_algo == "K-means":
                            # è®¾ç½® K å€¼
                            k_value1 = st.slider("é€‰æ‹© K-means çš„ K å€¼ï¼ˆç°‡çš„æ•°é‡ï¼‰", 2, 10, 3)
                            
                            # æ‰§è¡Œ K-means
                            kmeans1 = KMeans(n_clusters=k_value1, random_state=42, n_init=10)
                            # æ›¿æ¢ä¸º:
                            labels = kmeans1.fit_predict(X_scaled1)
                            df['Cluster1'] = [convert_cluster_label(str(label), naming_style1) for label in labels]                        
                            # è®¡ç®—è½®å»“ç³»æ•°
                            silhouette_avg = silhouette_score(X_scaled1, df['Cluster1'])
                            st.write(f"è½®å»“ç³»æ•° (è¶Šæ¥è¿‘1è¶Šå¥½): {silhouette_avg:.4f}")
                        
                        else:  # DBSCAN
                            # è®¾ç½® DBSCAN å‚æ•°å¹¶æ·»åŠ é€šä¿—è¯´æ˜
                            st.markdown("""
                            **DBSCAN å‚æ•°è¯´æ˜**:
                            - **eps**: ç‚¹çš„é‚»åŸŸåŠå¾„ï¼Œå†³å®šå¤šè¿œçš„ç‚¹è¢«è®¤ä¸ºæ˜¯â€œé‚»å±…â€ã€‚å€¼è¶Šå°ï¼Œç°‡è¶Šå¯†é›†ï¼›å€¼è¶Šå¤§ï¼Œç°‡è¶Šåˆ†æ•£ã€‚
                            - **min_samples**: ä¸€ä¸ªç°‡æ‰€éœ€çš„æœ€å°ç‚¹æ•°ï¼ˆåŒ…æ‹¬æ ¸å¿ƒç‚¹æœ¬èº«ï¼‰ã€‚å€¼è¶Šå¤§ï¼Œç°‡éœ€è¦æ›´å¤šç‚¹æ‰èƒ½å½¢æˆï¼›å€¼è¶Šå°ï¼Œå¯èƒ½ç”Ÿæˆæ›´å¤šå°ç°‡ã€‚
                            """)
                            eps = st.slider("é€‰æ‹© DBSCAN çš„ eps å‚æ•°", 0.1, 2.0, 0.5, step=0.01)
                            min_samples = st.slider("é€‰æ‹© DBSCAN çš„ min_samples å‚æ•°", 2, 20, 5)
                            
                            # æ‰§è¡Œ DBSCAN
                            dbscan1 = DBSCAN(eps=eps, min_samples=min_samples)
                            labels = dbscan1.fit_predict(X_scaled1)
                            
                            # å¤„ç†å™ªå£°ç‚¹ (-1)
                            if -1 in labels:
                                # æ‰¾åˆ°éå™ªå£°ç‚¹
                                non_noise_mask = labels != -1
                                X_non_noise = X_scaled1[non_noise_mask]
                                labels_non_noise = labels[non_noise_mask]
                                
                                # å¯¹å™ªå£°ç‚¹ä½¿ç”¨ KNN åˆ†é…æœ€è¿‘çš„ç°‡
                                noise_mask = labels == -1
                                X_noise = X_scaled1[noise_mask]
                                
                                if len(X_non_noise) > 0 and len(X_noise) > 0:
                                    knn = NearestNeighbors(n_neighbors=1)
                                    knn.fit(X_non_noise)
                                    _, indices = knn.kneighbors(X_noise)
                                    labels[noise_mask] = labels_non_noise[indices.flatten()]
                            
                            df['Cluster1'] = [convert_cluster_label(str(label), naming_style1) for label in labels]

                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç°‡
                            if len(np.unique(df['Cluster1'])) > 1:
                                silhouette_avg = silhouette_score(X_scaled1, df['Cluster1'])
                                st.write(f"è½®å»“ç³»æ•° (è¶Šæ¥è¿‘1è¶Šå¥½): {silhouette_avg:.4f}")
                            else:
                                st.warning("DBSCAN æœªèƒ½ç”Ÿæˆå¤šä¸ªæœ‰æ•ˆç°‡ï¼Œè¯·è°ƒæ•´ eps æˆ– min_samples å‚æ•°")
                        
                        # æ˜¾ç¤ºèšç±»ç»“æœ
                        st.write("èšç±»ç»“æœåˆ†å¸ƒ:")
                        cluster_counts1 = df['Cluster1'].value_counts().sort_index()
                        st.write(cluster_counts1)
                        
                        # å¯è§†åŒ–èšç±»ç»“æœ
                        if len(selected_cols1) == 1:
                            # ä¸€ç»´ï¼šåˆ†å¸ƒå›¾ + ç°‡åˆ†å‰²çº¿
                            fig, ax = plt.subplots(figsize=(10, 6))
                            unique_clusters = sorted(df['Cluster1'].unique())
                            colors = sns.color_palette("husl", len(unique_clusters))
                            
                            for cluster, color in zip(unique_clusters, colors):
                                cluster_data = X1[selected_cols1[0]][df['Cluster1'] == cluster]
                                sns.histplot(cluster_data, kde=True, label=f'ç°‡ {cluster}', 
                                        stat='density', alpha=0.4, color=color, ax=ax)
                            
                            # æ·»åŠ ç°‡åˆ†å‰²çº¿ï¼ˆåŸºäºç°‡ä¸­å¿ƒï¼‰
                            cluster_centers = []
                            for cluster in unique_clusters:
                                cluster_data = X1[selected_cols1[0]][df['Cluster1'] == cluster]
                                if len(cluster_data) > 0:
                                    cluster_centers.append(cluster_data.mean())
                            cluster_centers.sort()
                            
                            for center in cluster_centers:
                                ax.axvline(center, color='black', linestyle='--', alpha=0.5)
                            
                            plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„åˆ†å¸ƒä¸ç°‡åˆ†å‰²')
                            plt.xlabel(selected_cols1[0])
                            plt.ylabel('å¯†åº¦')
                            plt.legend()
                            st.pyplot(fig)
                        
                        elif len(selected_cols1) == 2 or len(selected_cols1) > 2:
                            # å¤šç»´ï¼šé€‰æ‹©å¯è§†åŒ–æ–¹å¼
                            vis_option1 = st.radio(
                                "é€‰æ‹©å¯è§†åŒ–æ–¹å¼ï¼ˆç¬¬ä¸€ä¸ªèšç±»ï¼‰",
                                ["åŸºäºåŸå§‹ç»´åº¦", "t-SNEé™ç»´åˆ°2ç»´"],
                                index=0
                            )
                            
                            if vis_option1 == "t-SNEé™ç»´åˆ°2ç»´":
                                # t-SNE é™ç»´åˆ° 2 ç»´
                                tsne = TSNE(n_components=2, random_state=42)
                                X_tsne = tsne.fit_transform(X_scaled1)
                                
                                fig, ax = plt.subplots(figsize=(10, 6))
                                sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], 
                                            hue=df['Cluster1'], palette='husl', ax=ax)
                                plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„ t-SNE é™ç»´åˆ†å¸ƒ')
                                plt.xlabel('t-SNE ç»´åº¦ 1')
                                plt.ylabel('t-SNE ç»´åº¦ 2')
                                st.pyplot(fig)
                            else:
                                # åŸºäºåŸå§‹ç»´åº¦ï¼ˆå–å‰ä¸¤ä¸ªç»´åº¦ï¼‰
                                fig, ax = plt.subplots(figsize=(10, 6))
                                if len(selected_cols1) >= 2:
                                    sns.scatterplot(x=X1[selected_cols1[0]], y=X1[selected_cols1[1]], 
                                                hue=df['Cluster1'], palette='husl', ax=ax)
                                    plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„å‰ä¸¤ä¸ªç»´åº¦åˆ†å¸ƒ')
                                    plt.xlabel(selected_cols1[0])
                                    plt.ylabel(selected_cols1[1])
                                else:
                                    sns.histplot(X1[selected_cols1[0]], hue=df['Cluster1'], palette='husl', ax=ax)
                                    plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„å•ç»´åº¦åˆ†å¸ƒ')
                                    plt.xlabel(selected_cols1[0])
                                st.pyplot(fig)
                        
                        else:
                            # å¤šç»´ï¼šé€‰æ‹©å¯è§†åŒ–æ–¹å¼
                            vis_option1 = st.radio(
                                "é€‰æ‹©å¯è§†åŒ–æ–¹å¼ï¼ˆç¬¬ä¸€ä¸ªèšç±»ï¼‰",
                                ["åŸºäºåŸå§‹ç»´åº¦", "t-SNEé™ç»´åˆ°2ç»´"],
                                index=0
                            )
                            
                            if vis_option1 == "t-SNEé™ç»´åˆ°2ç»´":
                                # t-SNE é™ç»´åˆ° 2 ç»´
                                tsne = TSNE(n_components=2, random_state=42)
                                X_tsne = tsne.fit_transform(X_scaled1)
                                
                                fig, ax = plt.subplots(figsize=(10, 6))
                                sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], 
                                            hue=df['Cluster1'], palette='husl', ax=ax)
                                plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„ t-SNE é™ç»´åˆ†å¸ƒ')
                                plt.xlabel('t-SNE ç»´åº¦ 1')
                                plt.ylabel('t-SNE ç»´åº¦ 2')
                                st.pyplot(fig)
                            else:
                                # åŸºäºåŸå§‹ç»´åº¦ï¼ˆå–å‰ä¸¤ä¸ªç»´åº¦ï¼‰
                                fig, ax = plt.subplots(figsize=(10, 6))
                                sns.scatterplot(x=X1[selected_cols1[0]], y=X1[selected_cols1[1]], 
                                            hue=df['Cluster1'], palette='husl', ax=ax)
                                plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„å‰ä¸¤ä¸ªç»´åº¦åˆ†å¸ƒ')
                                plt.xlabel(selected_cols1[0])
                                plt.ylabel(selected_cols1[1])
                                st.pyplot(fig)
                        
                        cluster_col1 = 'Cluster1'
                    else:
                        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€åˆ—ç”¨äºèšç±»")
                else:
                    st.error("æ•°æ®ä¸­æ²¡æœ‰æ•°å€¼å‹åˆ—ï¼Œæ— æ³•æ‰§è¡Œèšç±»")
            else:
                # ä½¿ç”¨å·²æœ‰èšç±»åˆ—
                all_cols = df.columns.tolist()
                cluster_col1 = st.selectbox("é€‰æ‹©ç¬¬ä¸€ä¸ªèšç±»åˆ—", all_cols)
                
                if cluster_col1:
                    df[cluster_col1] = df[cluster_col1].astype(str)  # ç¡®ä¿ä¸ºåˆ†ç±»å˜é‡
                    # æ˜¾ç¤ºèšç±»ç»“æœ
                    st.write("èšç±»ç»“æœåˆ†å¸ƒ:")
                    cluster_counts1 = df[cluster_col1].value_counts().sort_index()
                    st.write(cluster_counts1)
                    
                    # ç»˜åˆ¶èšç±»åˆ†å¸ƒå›¾
                    fig, ax = plt.subplots(figsize=(10, 6))
                    cluster_counts1.plot(kind='bar', ax=ax)
                    plt.title('ç¬¬ä¸€ä¸ªèšç±»çš„ç°‡åˆ†å¸ƒ')
                    plt.xlabel('ç°‡ç¼–å·')
                    plt.ylabel('æ ·æœ¬æ•°é‡')
                    st.pyplot(fig)
            
            # ç¬¬äºŒä¸ªèšç±»
            st.subheader("ç¬¬äºŒä¸ªèšç±»")
            
            clustering_mode2 = st.radio(
                "é€‰æ‹©ç¬¬äºŒä¸ªèšç±»æ¨¡å¼:",
                ["ä½¿ç”¨æ¨¡å‹èšç±»", "ä½¿ç”¨å·²æœ‰èšç±»åˆ—"],
                key="clustering_mode2"
            )
            
            if clustering_mode2 == "ä½¿ç”¨æ¨¡å‹èšç±»":
                # é€‰æ‹©èšç±»ç®—æ³•
                clustering_algo2 = st.selectbox("é€‰æ‹©èšç±»ç®—æ³•", ["K-means", "DBSCAN"], key="clustering_algo2")
                # æ·»åŠ ç°‡å‘½åé£æ ¼é€‰æ‹©
                naming_style2 = st.selectbox(
                    "é€‰æ‹©ç°‡å‘½åé£æ ¼",
                    ["å­—æ¯", "æ•°å­—", "ä¸­æ–‡"],
                    index=0,  # é»˜è®¤é€‰æ‹©å­—æ¯
                    key="naming_style2"
                )
                # é€‰æ‹©ç”¨äºèšç±»çš„åˆ—
                numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
                if len(numeric_cols) > 0:
                    selected_cols2 = st.multiselect(
                        "é€‰æ‹©ç”¨äºç¬¬äºŒä¸ªèšç±»çš„åˆ—",
                        numeric_cols,
                        default=numeric_cols[-max(0,min(3, len(numeric_cols))):]
                    )
                    
                    if selected_cols2:
                        X2 = df[selected_cols2]
                        # æ ‡å‡†åŒ–æ•°æ®
                        scaler = StandardScaler()
                        X_scaled2 = scaler.fit_transform(X2)
                        
                        # å¤šç»´æ•°æ®é™ç»´é€‰é¡¹
                        if len(selected_cols2) >= 2:
                            input_option2 = st.radio(
                                "é€‰æ‹©èšç±»è¾“å…¥æ•°æ®ï¼ˆç¬¬äºŒä¸ªèšç±»ï¼‰",
                                ["åŸå§‹æ•°æ®", "t-SNEé™ç»´åˆ°2ç»´"],
                                index=0
                            )
                            if input_option2 == "t-SNEé™ç»´åˆ°2ç»´":
                                tsne = TSNE(n_components=2, random_state=42)
                                X_scaled2 = tsne.fit_transform(X_scaled2)
                        
                        if clustering_algo2 == "K-means":
                            # è®¾ç½® K å€¼
                            k_value2 = st.slider("é€‰æ‹© K-means çš„ K å€¼ï¼ˆç°‡çš„æ•°é‡ï¼‰", 2, 10, 3, key="k_value2")
                            
                            # æ‰§è¡Œ K-means
                            kmeans2 = KMeans(n_clusters=k_value2, random_state=42, n_init=10)
                            # æ›¿æ¢ä¸º:
                            labels = kmeans2.fit_predict(X_scaled2)
                            df['Cluster2'] = [convert_cluster_label(str(label), naming_style2) for label in labels]

                            # è®¡ç®—è½®å»“ç³»æ•°
                            silhouette_avg = silhouette_score(X_scaled2, df['Cluster2'])
                            st.write(f"è½®å»“ç³»æ•° (è¶Šæ¥è¿‘1è¶Šå¥½): {silhouette_avg:.4f}")
                        
                        else:  # DBSCAN
                            # è®¾ç½® DBSCAN å‚æ•°å¹¶æ·»åŠ é€šä¿—è¯´æ˜
                            st.markdown("""
                            **DBSCAN å‚æ•°è¯´æ˜**:
                            - **eps**: ç‚¹çš„é‚»åŸŸåŠå¾„ï¼Œå†³å®šå¤šè¿œçš„ç‚¹è¢«è®¤ä¸ºæ˜¯â€œé‚»å±…â€ã€‚å€¼è¶Šå°ï¼Œç°‡è¶Šå¯†é›†ï¼›å€¼è¶Šå¤§ï¼Œç°‡è¶Šåˆ†æ•£ã€‚
                            - **min_samples**: ä¸€ä¸ªç°‡æ‰€éœ€çš„æœ€å°ç‚¹æ•°ï¼ˆåŒ…æ‹¬æ ¸å¿ƒç‚¹æœ¬èº«ï¼‰ã€‚å€¼è¶Šå¤§ï¼Œç°‡éœ€è¦æ›´å¤šç‚¹æ‰èƒ½å½¢æˆï¼›å€¼è¶Šå°ï¼Œå¯èƒ½ç”Ÿæˆæ›´å¤šå°ç°‡ã€‚
                            """)
                            eps2 = st.slider("é€‰æ‹© DBSCAN çš„ eps å‚æ•°", 0.1, 2.0, 0.5, step=0.1, key="eps2")
                            min_samples2 = st.slider("é€‰æ‹© DBSCAN çš„ min_samples å‚æ•°", 2, 20, 5, key="min_samples2")
                            
                            # æ‰§è¡Œ DBSCAN
                            dbscan2 = DBSCAN(eps=eps2, min_samples=min_samples2)
                            labels = dbscan2.fit_predict(X_scaled2)
                            
                            # å¤„ç†å™ªå£°ç‚¹ (-1)
                            if -1 in labels:
                                # æ‰¾åˆ°éå™ªå£°ç‚¹
                                non_noise_mask = labels != -1
                                X_non_noise = X_scaled2[non_noise_mask]
                                labels_non_noise = labels[non_noise_mask]
                                
                                # å¯¹å™ªå£°ç‚¹ä½¿ç”¨ KNN åˆ†é…æœ€è¿‘çš„ç°‡
                                noise_mask = labels == -1
                                X_noise = X_scaled2[noise_mask]
                                
                                if len(X_non_noise) > 0 and len(X_noise) > 0:
                                    knn = NearestNeighbors(n_neighbors=1)
                                    knn.fit(X_non_noise)
                                    _, indices = knn.kneighbors(X_noise)
                                    labels[noise_mask] = labels_non_noise[indices.flatten()]
                            
                            df['Cluster2'] = [convert_cluster_label(str(label), naming_style2) for label in labels]
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç°‡
                            if len(np.unique(df['Cluster2'])) > 1:
                                silhouette_avg = silhouette_score(X_scaled2, df['Cluster2'])
                                st.write(f"è½®å»“ç³»æ•° (è¶Šæ¥è¿‘1è¶Šå¥½): {silhouette_avg:.4f}")
                            else:
                                st.warning("DBSCAN æœªèƒ½ç”Ÿæˆå¤šä¸ªæœ‰æ•ˆç°‡ï¼Œè¯·è°ƒæ•´ eps æˆ– min_samples å‚æ•°")
                        
                        # æ˜¾ç¤ºèšç±»ç»“æœ
                        st.write("èšç±»ç»“æœåˆ†å¸ƒ:")
                        cluster_counts2 = df['Cluster2'].value_counts().sort_index()
                        st.write(cluster_counts2)
                        
                        # å¯è§†åŒ–èšç±»ç»“æœ
                        if len(selected_cols2) == 1:
                            # ä¸€ç»´ï¼šåˆ†å¸ƒå›¾ + ç°‡åˆ†å‰²çº¿
                            fig, ax = plt.subplots(figsize=(10, 6))
                            unique_clusters = sorted(df['Cluster2'].unique())
                            colors = sns.color_palette("husl", len(unique_clusters))
                            
                            for cluster, color in zip(unique_clusters, colors):
                                cluster_data = X2[selected_cols2[0]][df['Cluster2'] == cluster]
                                sns.histplot(cluster_data, kde=True, label=f'ç°‡ {cluster}', 
                                        stat='density', alpha=0.4, color=color, ax=ax)
                            
                            # æ·»åŠ ç°‡åˆ†å‰²çº¿ï¼ˆåŸºäºç°‡ä¸­å¿ƒï¼‰
                            cluster_centers = []
                            for cluster in unique_clusters:
                                cluster_data = X2[selected_cols2[0]][df['Cluster2'] == cluster]
                                if len(cluster_data) > 0:
                                    cluster_centers.append(cluster_data.mean())
                            cluster_centers.sort()
                            
                            for center in cluster_centers:
                                ax.axvline(center, color='black', linestyle='--', alpha=0.5)
                            
                            plt.title('ç¬¬äºŒä¸ªèšç±»çš„åˆ†å¸ƒä¸ç°‡åˆ†å‰²')
                            plt.xlabel(selected_cols2[0])
                            plt.ylabel('å¯†åº¦')
                            plt.legend()
                            st.pyplot(fig)
                        
                        elif len(selected_cols2) == 2 or len(selected_cols2) > 2:
                            # å¤šç»´ï¼šé€‰æ‹©å¯è§†åŒ–æ–¹å¼
                            vis_option2 = st.radio(
                                "é€‰æ‹©å¯è§†åŒ–æ–¹å¼ï¼ˆç¬¬äºŒä¸ªèšç±»ï¼‰",
                                ["åŸºäºåŸå§‹ç»´åº¦", "t-SNEé™ç»´åˆ°2ç»´"],
                                index=0
                            )
                            
                            if vis_option2 == "t-SNEé™ç»´åˆ°2ç»´":
                                # t-SNE é™ç»´åˆ° 2 ç»´
                                tsne = TSNE(n_components=2, random_state=42)
                                X_tsne = tsne.fit_transform(X_scaled2)
                                
                                fig, ax = plt.subplots(figsize=(10, 6))
                                sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], 
                                            hue=df['Cluster2'], palette='husl', ax=ax)
                                plt.title('ç¬¬äºŒä¸ªèšç±»çš„ t-SNE é™ç»´åˆ†å¸ƒ')
                                plt.xlabel('t-SNE ç»´åº¦ 1')
                                plt.ylabel('t-SNE ç»´åº¦ 2')
                                st.pyplot(fig)
                            else:
                                # åŸºäºåŸå§‹ç»´åº¦ï¼ˆå–å‰ä¸¤ä¸ªç»´åº¦ï¼‰
                                fig, ax = plt.subplots(figsize=(10, 6))
                                if len(selected_cols2) >= 2:
                                    sns.scatterplot(x=X2[selected_cols2[0]], y=X2[selected_cols2[1]], 
                                                hue=df['Cluster2'], palette='husl', ax=ax)
                                    plt.title('ç¬¬äºŒä¸ªèšç±»çš„å‰ä¸¤ä¸ªç»´åº¦åˆ†å¸ƒ')
                                    plt.xlabel(selected_cols2[0])
                                    plt.ylabel(selected_cols2[1])
                                else:
                                    sns.histplot(X2[selected_cols2[0]], hue=df['Cluster2'], palette='husl', ax=ax)
                                    plt.title('ç¬¬äºŒä¸ªèšç±»çš„å•ç»´åº¦åˆ†å¸ƒ')
                                    plt.xlabel(selected_cols2[0])
                                st.pyplot(fig)
                        
                        else:
                            # å¤šç»´ï¼šé€‰æ‹©å¯è§†åŒ–æ–¹å¼
                            vis_option2 = st.radio(
                                "é€‰æ‹©å¯è§†åŒ–æ–¹å¼ï¼ˆç¬¬äºŒä¸ªèšç±»ï¼‰",
                                ["åŸºäºåŸå§‹ç»´åº¦", "t-SNEé™ç»´åˆ°2ç»´"],
                                index=0
                            )
                            
                            if vis_option2 == "t-SNEé™ç»´åˆ°2ç»´":
                                # t-SNE é™ç»´åˆ° 2 ç»´
                                tsne = TSNE(n_components=2, random_state=42)
                                X_tsne = tsne.fit_transform(X_scaled2)
                                
                                fig, ax = plt.subplots(figsize=(10, 6))
                                sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], 
                                            hue=df['Cluster2'], palette='husl', ax=ax)
                                plt.title('ç¬¬äºŒä¸ªèšç±»çš„ t-SNE é™ç»´åˆ†å¸ƒ')
                                plt.xlabel('t-SNE ç»´åº¦ 1')
                                plt.ylabel('t-SNE ç»´åº¦ 2')
                                st.pyplot(fig)
                            else:
                                # åŸºäºåŸå§‹ç»´åº¦ï¼ˆå–å‰ä¸¤ä¸ªç»´åº¦ï¼‰
                                fig, ax = plt.subplots(figsize=(10, 6))
                                sns.scatterplot(x=X2[selected_cols2[0]], y=X2[selected_cols2[1]], 
                                            hue=df['Cluster2'], palette='husl', ax=ax)
                                plt.title('ç¬¬äºŒä¸ªèšç±»çš„å‰ä¸¤ä¸ªç»´åº¦åˆ†å¸ƒ')
                                plt.xlabel(selected_cols2[0])
                                plt.ylabel(selected_cols2[1])
                                st.pyplot(fig)
                        
                        cluster_col2 = 'Cluster2'
                    else:
                        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€åˆ—ç”¨äºèšç±»")
                else:
                    st.error("æ•°æ®ä¸­æ²¡æœ‰æ•°å€¼å‹åˆ—ï¼Œæ— æ³•æ‰§è¡Œèšç±»")
            else:
                # ä½¿ç”¨å·²æœ‰èšç±»åˆ—
                all_cols = df.columns.tolist()
                if cluster_col1 in all_cols:
                    all_cols.remove(cluster_col1)
                
                cluster_col2 = st.selectbox("é€‰æ‹©ç¬¬äºŒä¸ªèšç±»åˆ—", all_cols)
                
                if cluster_col2:
                    df[cluster_col2] = df[cluster_col2].astype(str)  # ç¡®ä¿ä¸ºåˆ†ç±»å˜é‡
                    # æ˜¾ç¤ºèšç±»ç»“æœ
                    st.write("èšç±»ç»“æœåˆ†å¸ƒ:")
                    cluster_counts2 = df[cluster_col2].value_counts().sort_index()
                    st.write(cluster_counts2)
                    
                    # ç»˜åˆ¶èšç±»åˆ†å¸ƒå›¾
                    fig, ax = plt.subplots(figsize=(10, 6))
                    cluster_counts2.plot(kind='bar', ax=ax)
                    plt.title('ç¬¬äºŒä¸ªèšç±»çš„ç°‡åˆ†å¸ƒ')
                    plt.xlabel('ç°‡ç¼–å·')
                    plt.ylabel('æ ·æœ¬æ•°é‡')
                    st.pyplot(fig)
            
            # çƒ­åŠ›å›¾åˆ†æ
            if cluster_col1 and cluster_col2:
                st.subheader("çƒ­åŠ›å›¾åˆ†æ")
                
                # åˆ›å»ºäº¤å‰è¡¨
                crosstab = pd.crosstab(df[cluster_col1], df[cluster_col2])
                
                # è®¡ç®—æœŸæœ›é¢‘ç‡
                chi2, p, dof, expected = chi2_contingency(crosstab)
                
                # è®¡ç®—è°ƒæ•´åæ®‹å·®
                observed = crosstab.values
                expected = expected.reshape(observed.shape)
                
                # è®¡ç®—æ®‹å·®
                residuals = observed - expected
                
                # è®¡ç®—è°ƒæ•´åæ®‹å·®
                n = observed.sum()
                row_sums = observed.sum(axis=1).reshape(-1, 1)
                col_sums = observed.sum(axis=0).reshape(1, -1)
                
                adj_residuals = residuals / np.sqrt(
                    expected * (1 - row_sums / n) * (1 - col_sums / n)
                )
                
                # åˆ›å»ºè°ƒæ•´åæ®‹å·®çš„ DataFrame
                adj_residuals_df = pd.DataFrame(
                    adj_residuals,
                    index=crosstab.index,
                    columns=crosstab.columns
                )
                
                # æ˜¾ç¤ºäº¤å‰è¡¨
                st.write("äº¤å‰è¡¨ (è§‚å¯Ÿå€¼):")
                st.dataframe(crosstab)
                
                # ç»˜åˆ¶çƒ­åŠ›å›¾ï¼ˆè§‚å¯Ÿå€¼ï¼‰
                st.write("è§‚å¯Ÿå€¼çƒ­åŠ›å›¾:")
                fig, ax = plt.subplots(figsize=(12, 8))
                sns.heatmap(crosstab, annot=True, fmt="d", cmap="YlGnBu", ax=ax)
                plt.title(f'{cluster_col1} ä¸ {cluster_col2} çš„äº¤å‰çƒ­åŠ›å›¾')
                st.pyplot(fig)
                
                # ç»˜åˆ¶è°ƒæ•´åæ®‹å·®çƒ­åŠ›å›¾
                st.write("è°ƒæ•´åæ®‹å·®çƒ­åŠ›å›¾:")
                st.write("(å€¼ > 1.96 æˆ– < -1.96 è¡¨ç¤ºåœ¨95%ç½®ä¿¡åº¦ä¸‹ç»Ÿè®¡æ˜¾è‘—)")
                fig, ax = plt.subplots(figsize=(12, 8))
                sns.heatmap(adj_residuals_df, annot=True, fmt=".2f", cmap="coolwarm", center=0, ax=ax)
                plt.title(f'{cluster_col1} ä¸ {cluster_col2} çš„è°ƒæ•´åæ®‹å·®çƒ­åŠ›å›¾')
                st.pyplot(fig)
                
                # æ˜¾ç¤ºå¡æ–¹æ£€éªŒç»“æœ
                st.write(f"å¡æ–¹ç»Ÿè®¡é‡: {chi2:.4f}, på€¼: {p:.4f}")
                if p < 0.05:
                    st.write("ä¸¤ä¸ªèšç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—å…³è” (p < 0.05)")
                else:
                    st.write("ä¸¤ä¸ªèšç±»ä¹‹é—´ä¸å­˜åœ¨æ˜¾è‘—å…³è” (p >= 0.05)")
                
                # æŸ¥æ‰¾æœ€æ˜¾è‘—çš„ç»„åˆ
                st.subheader("æœ€æ˜¾è‘—çš„èšç±»ç»„åˆ:")
                flat_residuals = adj_residuals_df.abs().stack()  # ä½¿ç”¨ stack() æ›¿ä»£ unstack()
                top_significant = flat_residuals.sort_values(ascending=False).head(5)
                
                for idx, value in top_significant.items():
                    cluster1, cluster2 = idx
                    try:
                        observed_val = crosstab.loc[cluster1, cluster2]
                        expected_val = expected[crosstab.index.get_loc(cluster1), crosstab.columns.get_loc(cluster2)]
                        direction = "é«˜äº" if observed_val > expected_val else "ä½äº"
                        
                        st.write(f"èšç±»ç»„åˆ ({cluster_col1}={cluster1}, {cluster_col2}={cluster2}):")
                        st.write(f"  - è°ƒæ•´åæ®‹å·®: {adj_residuals_df.loc[cluster1, cluster2]:.4f}")
                        st.write(f"  - è§‚å¯Ÿè®¡æ•°: {observed_val} ({direction}æœŸæœ›)")
                        st.write(f"  - æœŸæœ›è®¡æ•°: {expected_val:.2f}")
                    except KeyError:
                        st.warning(f"èšç±»ç»„åˆ ({cluster_col1}={cluster1}, {cluster_col2}={cluster2})æ— æ•ˆï¼šç´¢å¼•ä¸å­˜åœ¨")
                
                # æä¾›ä¸‹è½½åŠŸèƒ½
                st.subheader("ä¸‹è½½åˆ†æç»“æœ")
                
                # å‡†å¤‡æ•°æ®
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®ä¸èšç±»', index=False)
                    crosstab.to_excel(writer, sheet_name='äº¤å‰è¡¨')
                    adj_residuals_df.to_excel(writer, sheet_name='è°ƒæ•´åæ®‹å·®')
                    
                    # æ·»åŠ æœ€æ˜¾è‘—ç»„åˆçš„è¡¨æ ¼
                    sig_data = []
                    for idx, value in top_significant.items():
                        cluster1, cluster2 = idx
                        try:
                            observed_val = crosstab.loc[cluster1, cluster2]
                            expected_val = expected[crosstab.index.get_loc(cluster1), crosstab.columns.get_loc(cluster2)]
                            sig_data.append({
                                f'{cluster_col1}': cluster1,
                                f'{cluster_col2}': cluster2,
                                'è°ƒæ•´åæ®‹å·®': adj_residuals_df.loc[cluster1, cluster2],
                                'è§‚å¯Ÿè®¡æ•°': observed_val,
                                'æœŸæœ›è®¡æ•°': expected_val,
                                'æ˜¯å¦æ˜¾è‘—': abs(adj_residuals_df.loc[cluster1, cluster2]) > 1.96
                            })
                        except KeyError:
                            continue
                    
                    pd.DataFrame(sig_data).to_excel(writer, sheet_name='æ˜¾è‘—ç»„åˆ')
                
                output.seek(0)
                
                # æä¾›ä¸‹è½½é“¾æ¥
                b64 = base64.b64encode(output.read()).decode()
                href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="èšç±»åˆ†æç»“æœ.xlsx">ä¸‹è½½Excelåˆ†æç»“æœ</a>'
                st.markdown(href, unsafe_allow_html=True)
                
        except Exception as e:
            import traceback
            st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            st.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    else:
        st.info("è¯·ä¸Šä¼  CSV æˆ– Excel æ–‡ä»¶ä»¥å¼€å§‹åˆ†æ")

# Modify the main() function to add the Sales Forecasting option
def main():
    st.set_page_config(layout="wide")

    st.markdown(
        """
        <h3 style='color: #800080; font-weight: bold; text-align: center;'>
            ğŸµ éšæ—¶éšåœ°è†å¬ç²¾å½©éŸ³é¢‘ï¼ğŸµ
        </h3>
        <p style='color: #800080; text-align: center;'>
            æ¢ç´¢ app çš„åŒæ—¶ï¼Œäº«å—ç‹¬å®¶åŒ»ç–—æ´å¯ŸéŸ³é¢‘ï¼ˆä¿¡æ¯æ¥æºäºç½‘ç»œï¼‰ï¼Œæå‡æ‚¨çš„ä½¿ç”¨ä½“éªŒï¼
        </p>
        """,
        unsafe_allow_html=True
    )
    
    # Audio selection dropdown
    audio_folder = "audio_folder"
    default_audio = "sample.mp3"
    audio_files = []
    
    # Check if audio_folder exists and list .mp3 files
    if os.path.exists(audio_folder):
        audio_files = [f for f in os.listdir(audio_folder) if f.endswith(".mp3")]
    
    # Add default audio to the list if it exists in the folder
    if audio_files and default_audio in audio_files:
        default_index = audio_files.index(default_audio)
    else:
        default_index = 0
        if not audio_files:
            audio_files = ["æ— éŸ³é¢‘æ–‡ä»¶å¯ç”¨"]
    
    # Audio dropdown menu
    selected_audio = st.selectbox(
        "é€‰æ‹©éŸ³é¢‘",
        audio_files,
        index=default_index,
        key="audio_select"
    )
    
    # Play selected audio
    if selected_audio != "æ— éŸ³é¢‘æ–‡ä»¶å¯ç”¨":
        audio_file_path = os.path.join(audio_folder, selected_audio)
        if os.path.exists(audio_file_path):
            st.audio(audio_file_path, format="audio/mp3", start_time=0)
            # st.markdown(
            #     "<p style='color: #00FF00; text-align: center; font-weight: bold;'>æ­£åœ¨æ’­æ”¾ä¸­...</p>",
            #     unsafe_allow_html=True
            # )
        else:
            st.error(f"éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {audio_file_path}")
    else:
        st.error("éŸ³é¢‘æ–‡ä»¶å¤¹ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ audio_folder/ ç›®å½•")
    
    # Create the page selection in sidebar
    page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["Medical Insights Copilot", "Spreadsheet Analysis", "Sales Forecasting","Cluster Analysis"])
    
    if page == "Medical Insights Copilot":  
        # model_choice, client = setup_client(model_choice = 'gemini-2.0-flash')
        model_choice, client = setup_client()
        setup_layout(
            topics, diseases, institutions, departments, persons,
            primary_topics_list, primary_diseases_list,
            generate_tag, generate_diseases_tag, rewrite,
            prob_identy, generate_structure_data,
            model_choice, client
        )
    elif page == "Spreadsheet Analysis":
        setup_spreadsheet_analysis()
    elif page == "Sales Forecasting":
        setup_sales_forecasting()
    elif page == "Cluster Analysis":
        setup_clustering_analysis()


if __name__ == "__main__":
    main()
