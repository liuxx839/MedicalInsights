#main.py
import streamlit as st
from layout import setup_layout
from functions import (
    setup_client, generate_tag, generate_diseases_tag, rewrite,
    prob_identy, generate_structure_data
)
from config import (
    topics, diseases, institutions, departments, persons,
    primary_topics_list, primary_diseases_list,colors
)
from streamlit_extras.stylable_container import stylable_container
import pandas as pd
import json
import time
import ast
import io
from openai import OpenAI
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from io import BytesIO
from dagrelation import DAGRelations
from datadescription import DataDescription

# Add at the top of the file with other imports
import numpy as np
from datetime import datetime
from prophet import Prophet
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import base64
from io import BytesIO
import re

model_choice_research, client_research = setup_client(model_choice = 'gemini-2.0-flash')
 
def create_mermaid_html_from_edges(dag_edges):
    """
    Create a Mermaid flowchart HTML from a list of edges.
    
    Parameters:
    dag_edges (list): List of tuples representing edges. Each tuple can be:
                     - ('var1', 'var2') for a single edge
                     - (['var1', 'var2'], 'var3') for multiple sources to one target
    
    Returns:
    str: HTML code with embedded Mermaid flowchart
    """
    mermaid_code = ['flowchart TD']
    
    for edge in dag_edges:
        if isinstance(edge[0], list):
            # Handle multiple sources to one target
            source_vars = edge[0]
            target_var = edge[1]
            
            # Create individual connections for each source to target
            for source_var in source_vars:
                mermaid_code.append(f'    {source_var} --> {target_var}')
        else:
            # Simple one-to-one edge
            source_var = edge[0]
            target_var = edge[1]
            mermaid_code.append(f'    {source_var} --> {target_var}')
    
    # Join all lines with newlines
    mermaid_code_str = '\n'.join(mermaid_code)
    
    # Create the HTML with the embedded Mermaid code and zoom controls
    mermaid_html = f"""
<div id="mermaid-container" style="width:100%; height:100%; position:relative;">
    <div class="mermaid" id="mermaid-diagram">
    {mermaid_code_str}
    </div>
    
    <div style="position:absolute; top:10px; right:10px; background:white; padding:5px; border:1px solid #ccc; border-radius:5px;">
        <button onclick="zoomIn()" style="margin-right:5px;">â•</button>
        <button onclick="zoomOut()">â–</button>
        <button onclick="resetZoom()" style="margin-left:5px;">ğŸ”„</button>
    </div>
</div>

<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({{ startOnLoad: true }});
    
    window.currentZoom = 1;
    
    window.zoomIn = function() {{
        window.currentZoom += 0.1;
        document.getElementById('mermaid-diagram').style.transform = `scale(${{window.currentZoom}})`;
        document.getElementById('mermaid-diagram').style.transformOrigin = 'top left';
    }};
    
    window.zoomOut = function() {{
        if (window.currentZoom > 0.5) {{
            window.currentZoom -= 0.1;
            document.getElementById('mermaid-diagram').style.transform = `scale(${{window.currentZoom}})`;
            document.getElementById('mermaid-diagram').style.transformOrigin = 'top left';
        }}
    }};
    
    window.resetZoom = function() {{
        window.currentZoom = 1;
        document.getElementById('mermaid-diagram').style.transform = 'scale(1)';
    }};
</script>
"""
    
    return mermaid_html
    
def create_word_document(qa_response, fact_check_result=None):
    """
    åˆ›å»ºåŒ…å«QAå“åº”å’Œäº‹å®æ ¸æŸ¥ç»“æœçš„Wordæ–‡æ¡£ï¼Œæ”¯æŒMarkdownæ ‡é¢˜è½¬æ¢ä¸ºWordæ ·å¼
    """
    doc = Document()
    
    # è®¾ç½®æ ‡é¢˜
    title = doc.add_heading('Medical Knowledge Base Q&A Report', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # æ·»åŠ æ—¶é—´æˆ³
    timestamp = doc.add_paragraph()
    timestamp.add_run(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}").italic = True
    
    # æ·»åŠ QAå“åº”
    doc.add_heading('å›åº”å†…å®¹', level=1)
    
    # å¤„ç†QAå“åº”ä¸­çš„Markdownæ ¼å¼
    lines = qa_response.split('\n')
    current_text = []
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
        if line.strip().startswith('#'):
            # å¦‚æœä¹‹å‰æœ‰ç´¯ç§¯çš„æ–‡æœ¬ï¼Œå…ˆæ·»åŠ ä¸ºæ®µè½
            if current_text:
                doc.add_paragraph(''.join(current_text))
                current_text = []
            
            # è®¡ç®—æ ‡é¢˜çº§åˆ«
            level = 1
            line = line.strip()
            while line.startswith('#'):
                level += 1
                line = line[1:]
            level = min(level, 9)  # Wordæ”¯æŒæœ€å¤š9çº§æ ‡é¢˜
            
            # ç§»é™¤å¯èƒ½çš„å¼•ç”¨æ ‡è®° [1,2,3] å¹¶ä¿å­˜
            citation = ''
            if '[' in line and ']' in line:
                main_text = line[:line.find('[')].strip()
                citation = line[line.find('['):].strip()
                heading = doc.add_heading(main_text, level=level-1)
                if citation:
                    heading.add_run(f" {citation}").italic = True
            else:
                doc.add_heading(line.strip(), level=level-1)
        else:
            current_text.append(line + '\n')
    
    # æ·»åŠ æœ€åå‰©ä½™çš„æ–‡æœ¬
    if current_text:
        doc.add_paragraph(''.join(current_text))
    
    # å¦‚æœæœ‰äº‹å®æ ¸æŸ¥ç»“æœï¼Œæ·»åŠ åˆ°æ–‡æ¡£
    if fact_check_result:
        doc.add_heading('äº‹å®æ ¸æŸ¥ç»“æœ', level=1)
        # å¯¹äº‹å®æ ¸æŸ¥ç»“æœä¹Ÿè¿›è¡Œç›¸åŒçš„å¤„ç†
        lines = fact_check_result.split('\n')
        current_text = []
        
        for line in lines:
            if line.strip().startswith('#'):
                if current_text:
                    doc.add_paragraph(''.join(current_text))
                    current_text = []
                
                level = 1
                line = line.strip()
                while line.startswith('#'):
                    level += 1
                    line = line[1:]
                level = min(level, 9)
                
                if '[' in line and ']' in line:
                    main_text = line[:line.find('[')].strip()
                    citation = line[line.find('['):].strip()
                    heading = doc.add_heading(main_text, level=level-1)
                    if citation:
                        heading.add_run(f" {citation}").italic = True
                else:
                    doc.add_heading(line.strip(), level=level-1)
            else:
                current_text.append(line + '\n')
        
        if current_text:
            doc.add_paragraph(''.join(current_text))
    
    # ä¿å­˜åˆ°å†…å­˜ä¸­
    doc_io = io.BytesIO()
    doc.save(doc_io)
    doc_io.seek(0)
    return doc_io

def extract_dag_edges(text_content):
    # ä»æ–‡æœ¬ä¸­æå– dag_edges éƒ¨åˆ†
    if "dag_edges = [" in text_content:
        start_idx = text_content.find("dag_edges = [")
        start_idx = text_content.find("[", start_idx)
        
        # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·
        open_brackets = 1
        end_idx = start_idx + 1
        
        while open_brackets > 0 and end_idx < len(text_content):
            if text_content[end_idx] == '[':
                open_brackets += 1
            elif text_content[end_idx] == ']':
                open_brackets -= 1
            end_idx += 1
        
        # æå– dag_edges åˆ—è¡¨å†…å®¹
        dag_edges_str = text_content[start_idx:end_idx]
        
        # ä½¿ç”¨ ast.literal_eval å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º Python å¯¹è±¡
        try:
            dag_edges = ast.literal_eval(dag_edges_str)
            return dag_edges
        except (SyntaxError, ValueError) as e:
            print(f"è§£æé”™è¯¯: {e}")
            return None
    
    return None

def setup_spreadsheet_analysis():
    st.markdown(
        """
    <h1 style='text-align: center; font-size: 18px; font-weight: bold;'>Spreadsheet Analysis</h1>
    <h6 style='text-align: center; font-size: 12px;'>ä¸Šä¼ Excel/CSVæ–‡ä»¶æˆ–ç²˜è´´JSONæ•°æ®è¿›è¡Œåˆ†æ</h6>
    <br><br><br>
    """,
        unsafe_allow_html=True,
    )
    
    # Initialize all session state variables
    if "analysis_response" not in st.session_state:
        st.session_state.analysis_response = ""
    if "analysis_reasoning" not in st.session_state:
        st.session_state.analysis_reasoning = ""
    if "dag_edges" not in st.session_state:
        st.session_state.dag_edges = ""
    if "sample_data" not in st.session_state:
        st.session_state.sample_data = {}
    if "business_report" not in st.session_state:
        st.session_state.business_report = ""
    if "dag_report" not in st.session_state:
        st.session_state.dag_report = ""
    if "dag_reasoning" not in st.session_state:
        st.session_state.dag_reasoning = ""
    if "df" not in st.session_state:
        st.session_state.df = None
    
    # Create tabs for different input methods
    tab1, tab2 = st.tabs(["æ–‡ä»¶ä¸Šä¼ ", "JSONç²˜è´´"])
    
    with tab1:
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶(xlsx/csv)", type=['xlsx', 'csv'])
        
        if uploaded_file is not None:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ•°æ®
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                    
                # å¤„ç†åˆ—åï¼Œå°†ç©ºæ ¼å’Œç‰¹æ®Šç¬¦å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
                df.columns = [re.sub(r'[^\w]', '_', col) for col in df.columns]
                
                # ä¿å­˜DataFrameåˆ°session state
                st.session_state.df = df
                
                # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
                st.write("æ•°æ®é¢„è§ˆ:")
                st.dataframe(df)
                
                # ä¿å­˜sampleæ•°æ®åˆ°session state
                st.session_state.sample_data = df.sample(10).to_dict()
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    with tab2:
        json_input = st.text_area("ç²˜è´´JSONæ•°æ®:", height=200)
        process_json_button = st.button("å¤„ç†JSONæ•°æ®")
        
        if process_json_button and json_input:
            try:
                # æ¸…ç†JSONå­—ç¬¦ä¸² - å¦‚æœå‰åæœ‰é¢å¤–çš„å¼•å·ï¼Œå»æ‰å®ƒä»¬
                cleaned_json = json_input.strip()
                if cleaned_json.startswith('"') and cleaned_json.endswith('"'):
                    # å¦‚æœJSONè¢«é¢å¤–çš„å¼•å·åŒ…å›´ï¼Œå»æ‰è¿™äº›å¼•å·å¹¶å¤„ç†è½¬ä¹‰å­—ç¬¦
                    cleaned_json = cleaned_json[1:-1].replace('\\"', '"')
                
                # è§£æJSONæ•°æ®
                try:
                    json_data = json.loads(cleaned_json)
                    df = pd.DataFrame(json_data)
                except Exception as e:
                    # å¦‚æœç¬¬ä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œå°è¯•ä»¥Recordsæ ¼å¼è§£æ
                    try:
                        json_data = json.loads(cleaned_json)
                        if isinstance(json_data, list):
                            df = pd.DataFrame(json_data)
                        else:
                            df = pd.DataFrame([json_data])
                    except:
                        st.error(f"æ— æ³•è§£æJSONæ•°æ®: {str(e)}")
                        return
                
                # ä¿å­˜DataFrameåˆ°session state
                st.session_state.df = df
                
                # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
                st.write("æ•°æ®é¢„è§ˆ:")
                st.dataframe(df)
                
                # ä¿å­˜sampleæ•°æ®åˆ°session state
                st.session_state.sample_data = df.head(10).to_dict()
                
            except Exception as e:
                st.error(f"å¤„ç†JSONæ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    # åªæœ‰å½“DataFrameå¯ç”¨æ—¶æ‰æ˜¾ç¤ºä¸‹é¢çš„æ§ä»¶
    if st.session_state.df is not None:
        # ç”¨æˆ·è¾“å…¥éœ€æ±‚
        user_question = st.text_area("è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚:", height=100)
        
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼Œä¸€åˆ—æ”¾ç”ŸæˆæŒ‰é’®ï¼Œä¸€åˆ—æ”¾DAGåˆ†ææŒ‰é’®
        col1, col2 = st.columns(2)
        
        with col1:
            # æ ¹æ®æ˜¯å¦å·²ç»è¿›è¡Œè¿‡DAGåˆ†ææ¥æ˜¾ç¤ºä¸åŒçš„æŒ‰é’®æ–‡æœ¬
            button_text = "å†æ¬¡ç”Ÿæˆ" if "business_report" in st.session_state and st.session_state.business_report else "ç”Ÿæˆ"
            generate_button = st.button(button_text)
        
        with col2:
            with stylable_container(
                "dag_analysis_button",
                css_styles="""
                    button {
                        background-color: #FFA500;
                        color: white;
                    }""",
            ):
                dag_analysis_button = st.button("ğŸ“Š DAGåˆ†æ")
        
        # ç¡®ä¿æœ‰DataFrameå¯ç”¨äºåˆ†æ
        df = st.session_state.df
        
        # å¤„ç†ç”ŸæˆæŒ‰é’®ç‚¹å‡»
        if generate_button and user_question:
            with st.spinner("æ­£åœ¨åˆ†æ..."):
                # æ ¹æ®æ˜¯å¦å·²ç»æœ‰business_reportæ¥å†³å®šä½¿ç”¨å“ªä¸ªæç¤º
                if "business_report" in st.session_state and st.session_state.business_report:
                    # ä½¿ç”¨å·²æœ‰çš„business_reportä½œä¸ºè¾“å…¥ï¼Œé‡æ–°æ€è€ƒDAGç»“æ„
                    response = client_research.chat.completions.create(
                        model=model_choice_research,
                        messages=[
                            {
                                "role": "system",
                                "content": """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„sampleæ•°æ®å’Œå·²æœ‰çš„åˆ†ææŠ¥å‘Šï¼Œé‡æ–°æ€è€ƒå¹¶ä¼˜åŒ–DAGå…³ç³»ã€‚
                                å…³æ³¨ä»¥ä¸‹å‡ ç‚¹ï¼š
                                1. ä»”ç»†åˆ†æå·²æœ‰æŠ¥å‘Šä¸­çš„å‘ç°å’Œå…³ç³»
                                2. é‡æ–°è¯„ä¼°å¯èƒ½çš„å› æœå…³ç³»
                                3. æ„å»ºæ›´ä¼˜åŒ–çš„DAGè¾¹
                                4. æ”¯æŒå¤šå¯¹ä¸€çš„å…³ç³»
                                5. ç¡®ä¿ä½¿ç”¨çš„æ˜¯åŸå§‹çš„åˆ—åï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹
                                6. è€ƒè™‘å·²æœ‰åˆ†æä¸­å¯èƒ½è¢«å¿½ç•¥çš„å…³ç³»
                                7. ä¸è¦å¢åŠ ä»»ä½•ä¸å­˜åœ¨çš„åˆ—å

                                è¯·å…ˆæä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œç„¶åå†ç»™å‡ºä¼˜åŒ–åçš„DAGå®šä¹‰ã€‚

                                æœ€ç»ˆè¾“å‡ºæ ¼å¼å¿…é¡»å¦‚ä¸‹ï¼š
                                ##æ¨ç†è¿‡ç¨‹
                                [è¯¦ç»†çš„åˆ†ææ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¯¹å·²æœ‰åˆ†æçš„è¯„ä¼°]

                                ##å®šä¹‰DAGè¾¹ï¼ˆæ”¯æŒå¤šå¯¹ä¸€å…³ç³»ï¼‰
                                è¯·ä¸¥æ ¼éµå¾ªä¸‹é¢çš„æ ¼å¼ï¼Œä»”ç»†æ ¸å¯¹ï¼ŒåŠ¡å¿…ä¿è¯ä½¿ç”¨åŸå§‹åˆ—åï¼Œä»¥åŠæ­£ç¡®çš„æ‹¬å·
                                dag_edges = [
                                    ('var1', 'var2'),
                                    ('var3', 'var4'),
                                    # å¤šå¯¹ä¸€å…³ç³»ç¤ºä¾‹
                                    (['var1', 'var2'], 'var3'),
                                    (['var1', 'var2', 'var3'], 'var4')
                                ]
                                
                                ##åˆ†æè¯´æ˜ï¼š
                                [è¿™é‡Œæ˜¯å¯¹ä¼˜åŒ–åDAGç»“æ„çš„è§£é‡Šè¯´æ˜ï¼Œä»¥åŠä¸ä¹‹å‰åˆ†æçš„æ¯”è¾ƒ]
                                """
                            },
                            {
                                "role": "user",
                                "content": f"Sampleæ•°æ®ï¼š\n{st.session_state.sample_data}\n\nç”¨æˆ·éœ€æ±‚ï¼š{user_question}\n\nå·²æœ‰åˆ†ææŠ¥å‘Šï¼š\n{st.session_state.business_report}"
                            }
                        ],
                        temperature=0.7,
                        max_tokens=2000,
                        stream=True
                    )
                else:
                    # åŸæœ‰çš„æç¤ºï¼Œç”¨äºé¦–æ¬¡ç”Ÿæˆ
                    response = client_research.chat.completions.create(
                        model=model_choice_research,
                        messages=[
                            {
                                "role": "system", 
                                "content": """
                                ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„sampleæ•°æ®åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œå¹¶æ„å»ºæ½œåœ¨çš„DAGå…³ç³»ã€‚
                                å…³æ³¨ä»¥ä¸‹å‡ ç‚¹ï¼š
                                0. ç¡®ä¿å……åˆ†ç†è§£ç”¨æˆ·çš„éœ€æ±‚ï¼ŒDAGæ„å»ºå›´ç»•ç”¨æˆ·çš„éœ€æ±‚
                                1. ä»”ç»†åˆ†ææ•°æ®åˆ—ä¹‹é—´çš„å…³ç³»
                                2. è¯†åˆ«å¯èƒ½çš„å› æœå…³ç³»
                                3. æ„å»ºåˆé€‚çš„DAGè¾¹
                                4. æ”¯æŒå¤šå¯¹ä¸€çš„å…³ç³»
                                5. ç¡®ä¿ä½¿ç”¨çš„æ˜¯åŸå§‹çš„åˆ—åï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹
                                6. ä¸è¦å¢åŠ ä»»ä½•ä¸å­˜åœ¨çš„åˆ—å

                                æœ€ç»ˆè¾“å‡ºæ ¼å¼å¿…é¡»å¦‚ä¸‹ï¼š

                                ## æ¨ç†è¿‡ç¨‹
                                [é¦–å…ˆå¤è¿°ç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆ]
                                [è¯¦ç»†çš„åˆ†ææ¨ç†è¿‡ç¨‹]

                                ## å®šä¹‰DAGè¾¹ï¼ˆæ”¯æŒå¤šå¯¹ä¸€å…³ç³»ï¼‰
                                è¯·ä¸¥æ ¼éµå¾ªä¸‹é¢çš„æ ¼å¼ï¼Œä»”ç»†æ ¸å¯¹ï¼ŒåŠ¡å¿…ä¿è¯ä½¿ç”¨åŸå§‹åˆ—åï¼Œä»¥åŠæ­£ç¡®çš„æ‹¬å·
                                dag_edges = [
                                    ('var1', 'var2'),
                                    ('var3', 'var4'),
                                    # å¤šå¯¹ä¸€å…³ç³»ç¤ºä¾‹
                                    (['var1', 'var2'], 'var3'),
                                    (['var1', 'var2', 'var3'], 'var4')
                                ]


                                ## åˆ†æè¯´æ˜ï¼š
                                [è¿™é‡Œæ˜¯å¯¹DAGç»“æ„çš„è§£é‡Šè¯´æ˜ï¼ŒåŒ…å«åŸåˆ™éµå¾ªæƒ…å†µåˆ†æ]
                                """
                            },
                            {
                                "role": "user", 
                                "content": f"Sampleæ•°æ®ï¼š\n{st.session_state.sample_data}\n\nç”¨æˆ·éœ€æ±‚ï¼š{user_question}"
                            }
                        ],
                        temperature=0.7,
                        max_tokens=2000,
                        stream=True
                    )
                
                full_response = ""
                reasoning_content = ""
                
                # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçš„å®¹å™¨
                progress_container = st.empty()
                
                # å¤„ç†æµå¼å“åº”
                for chunk in response:
                    if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                        reasoning_content += chunk.choices[0].delta.reasoning_content
                        progress_container.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nå›ç­”ï¼š\n{full_response}")
                    elif hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        progress_container.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nå›ç­”ï¼š\n{full_response}")
                
                # ä»…æå–DAGå®šä¹‰éƒ¨åˆ†
                dag_definition = ""
                if "dag_edges = [" in full_response:
                    dag_start = full_response.find("dag_edges = [")
                    
                    # Find the matching closing bracket by counting opening and closing brackets
                    open_brackets = 1
                    dag_end = dag_start + len("dag_edges = [")
                    
                    while open_brackets > 0 and dag_end < len(full_response):
                        if full_response[dag_end] == '[':
                            open_brackets += 1
                        elif full_response[dag_end] == ']':
                            open_brackets -= 1
                        dag_end += 1
                    
                    dag_definition = full_response[dag_start:dag_end]
                
                # ä¿å­˜åˆ°session state
                st.session_state.analysis_response = full_response
                st.session_state.analysis_reasoning = reasoning_content
                st.session_state.dag_edges = dag_definition
                
                # æ¸…ç©ºè¿›åº¦å®¹å™¨
                progress_container.empty()
        
        # å¤„ç†DAGåˆ†ææŒ‰é’®ç‚¹å‡»
        if dag_analysis_button and st.session_state.dag_edges:
            with st.spinner("æ­£åœ¨è¿›è¡ŒDAGåˆ†æ..."):
                try:
                    # æ‰§è¡ŒDAGåˆ†æ
                    dag_edges_text = st.session_state.dag_edges
                    dag_edges = extract_dag_edges(dag_edges_text)
                    
                    full_response = ""
                    reasoning_content = ""

                    # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçš„å®¹å™¨
                    dag_progress = st.empty()
                    
                    if dag_edges:
                        # æ‰§è¡ŒDAGåˆ†æ
                        analyzer = DAGRelations(df, dag_edges)
                        dag_report = analyzer.analyze_relations().print_report()
                        st.session_state.dag_report = dag_report

                        # æ·»åŠ æ•°æ®æè¿°åˆ†æ
                        data_analyzer = DataDescription(df, include_histogram=False, string_threshold=10)
                        data_analyzer.analyze_data()
                        json_output = data_analyzer.to_json()
                        st.session_state.data_description = json_output

                        # ç”Ÿæˆå•†ä¸šæŠ¥å‘Š
                        response = client_research.chat.completions.create(
                            model=model_choice_research,
                            messages=[
                                {
                                    "role": "system",
                                    "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·åŸºäºæä¾›çš„åˆå§‹åˆ†æç»“æœã€DAGåˆ†ææŠ¥å‘Šå’Œæ•°æ®æè¿°ä¿¡æ¯ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚å¦‚æœDAGåˆ†ææŠ¥å‘Šä¸å­˜åœ¨ï¼Œè¯·åœ¨æŠ¥å‘Šä¸­è¯´æ˜è¿™ä¸€æƒ…å†µã€‚

                                        æŠ¥å‘Šç»“æ„åº”åŒ…å«ï¼š

                                        # æ‰§è¡Œæ‘˜è¦
                                        ç®€æ˜æ‰¼è¦åœ°æ€»ç»“å…³é”®å‘ç°å’Œå»ºè®®ï¼ˆä¸è¶…è¿‡200å­—ï¼‰
                                        
                                        # å…³é”®æ´å¯Ÿ
                                        ## ä¸»è¦å‘ç°
                                        - å‘ç°1: [ç®€æ˜æè¿°] - æ”¯æŒæ•°æ®: [ç›¸å…³ç»Ÿè®¡ç»“æœ]
                                        - å‘ç°2: [ç®€æ˜æè¿°] - æ”¯æŒæ•°æ®: [ç›¸å…³ç»Ÿè®¡ç»“æœ]
                                        - å‘ç°n: ...
                                        
                                        # æ•°æ®æ¦‚è§ˆ
                                        ## æ•°æ®åŸºæœ¬æƒ…å†µ
                                        ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ï¼š
                                        | æ•°æ®ç»´åº¦ | å€¼ |
                                        | --- | --- |
                                        | æ€»è®°å½•æ•° | X |
                                        | å˜é‡æ•°é‡ | X |
                                        | æ•°å€¼å‹å˜é‡ | Xä¸ª (åˆ—å‡ºåç§°) |
                                        | åˆ†ç±»å‹å˜é‡ | Xä¸ª (åˆ—å‡ºåç§°) |
                                        | ç¼ºå¤±å€¼æƒ…å†µ | æ€»ä½“ç™¾åˆ†æ¯”åŠä¸»è¦ç¼ºå¤±åˆ— |

                                        ## å…³é”®å˜é‡åˆ†æ
                                        é’ˆå¯¹é‡è¦å˜é‡ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ï¼š

                                        **æ•°å€¼å‹å˜é‡ç»Ÿè®¡**
                                        | å˜é‡å | å‡å€¼ | ä¸­ä½æ•° | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | å¼‚å¸¸å€¼æ¯”ä¾‹ |
                                        | --- | --- | --- | --- | --- | --- | --- |

                                        **åˆ†ç±»å‹å˜é‡ç»Ÿè®¡**
                                        | å˜é‡å | å”¯ä¸€å€¼æ•°é‡ | æœ€å¸¸è§ç±»åˆ«(å æ¯”) | ç†µå€¼(å½’ä¸€åŒ–) | åˆ†å¸ƒå‡è¡¡åº¦ |
                                        | --- | --- | --- | --- | --- | --- |

                                        # å˜é‡å…³ç³»åˆ†æ
                                        ## DAGå…³ç³»æ¦‚è¿°
                                        ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ä¸»è¦å˜é‡é—´çš„å…³ç³»ï¼š

                                        | å…³ç³»ç±»å‹ | æºå˜é‡ | ç›®æ ‡å˜é‡ | ç»Ÿè®¡é‡ | på€¼ | æ˜¾è‘—æ€§ | å…³ç³»å¼ºåº¦ |
                                        | --- | --- | --- | --- | --- | --- | --- |

                                        ## è¯¦ç»†å…³ç³»åˆ†æ
                                        é’ˆå¯¹æ¯ä¸ªé‡è¦å…³ç³»ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

                                        ### å…³ç³»: [æºå˜é‡] -> [ç›®æ ‡å˜é‡]
                                        **ç»Ÿè®¡ç»“æœ:**
                                        - å…³ç³»ç±»å‹: [åˆ†ç±»->æ•°å€¼/æ•°å€¼->æ•°å€¼/åˆ†ç±»->åˆ†ç±»]
                                        - ç»Ÿè®¡é‡: [Få€¼/ç›¸å…³ç³»æ•°/å¡æ–¹å€¼] = X
                                        - på€¼: X
                                        - æ˜¾è‘—æ€§: [é«˜åº¦æ˜¾è‘—/æ˜¾è‘—/ä¸æ˜¾è‘—]

                                        **ç±»åˆ«ç»Ÿè®¡:** (é€‚ç”¨äºåˆ†ç±»->æ•°å€¼å…³ç³»)
                                        | ç±»åˆ« | æ ·æœ¬æ•° | å‡å€¼ | æ ‡å‡†å·® | ä¸æ€»ä½“å‡å€¼å·®å¼‚ |
                                        | --- | --- | --- | --- | --- |

                                        **æ˜¾è‘—å·®å¼‚ç±»åˆ«:**
                                        - ç±»åˆ«X: å‡å€¼Y (æ¯”æ€»ä½“å‡å€¼[é«˜/ä½]Z%)

                                        # æŠ€æœ¯é™„å½•
                                        ## DAGåˆ†æå®Œæ•´æŠ¥å‘Š
                                        [æ­¤å¤„ç›´æ¥æ’å…¥åŸå§‹DAGæŠ¥å‘Šï¼Œä¸åšä¿®æ”¹]

                                        è¯·ç¡®ä¿ï¼š
                                        1. è¡¨æ ¼æ ¼å¼æ¸…æ™°ï¼Œæ•°æ®å¯¹é½
                                        2. æ‰€æœ‰ç»Ÿè®¡ç»“æœä¿ç•™é€‚å½“å°æ•°ä½æ•°(é€šå¸¸2-4ä½)
                                        3. å¯¹ç»Ÿè®¡æ˜¾è‘—æ€§ä½¿ç”¨æ ‡å‡†è¡¨ç¤º: *** p<0.001, ** p<0.01, * p<0.05, ns pâ‰¥0.05
                                        4. å¯¹éæŠ€æœ¯è¯»è€…è§£é‡Šç»Ÿè®¡æœ¯è¯­ï¼Œä½†ä¿æŒä¸“ä¸šæ€§
                                        5. é‡ç‚¹çªå‡ºå¼‚å¸¸å€¼ã€æ˜¾è‘—å·®å¼‚å’Œæœ‰å•†ä¸šä»·å€¼çš„å‘ç°
                                        6. æ‰€æœ‰ç»“è®ºå¿…é¡»æœ‰æ•°æ®æ”¯æŒï¼Œé¿å…è¿‡åº¦è§£è¯»
                                        7. å¯¹äºå¤æ‚å…³ç³»ï¼Œæä¾›ç®€æ˜çš„è§£é‡Šå’Œå¯èƒ½çš„å› æœæœºåˆ¶
                                        8. è¯·æä¾›å°½å¯èƒ½è¯¦ç»†çš„æ•°æ®æ´å¯Ÿå’Œä¸šåŠ¡å½±å“åˆ†æ"""
                                },
                                {
                                    "role": "user",
                                    "content": f"""
                                    åˆå§‹åˆ†æç»“æœï¼š{st.session_state.analysis_response}
                                    DAGåˆ†ææŠ¥å‘Šï¼š{dag_report}
                                    æ•°æ®æè¿°ä¿¡æ¯ï¼š{json_output}
                                    
                                    è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ•°æ®åˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šåº”ä¸¥æ ¼éµå¾ªç³»ç»Ÿæç¤ºä¸­çš„ç»“æ„å’Œæ ¼å¼è¦æ±‚ï¼Œç‰¹åˆ«æ³¨æ„ï¼š
                                    1. å……åˆ†åˆ©ç”¨DAGåˆ†ææŠ¥å‘Šä¸­çš„ç»Ÿè®¡ç»“æœï¼ŒåŒ…æ‹¬Få€¼ã€på€¼å’Œç±»åˆ«ç»Ÿè®¡
                                    2. æ•´åˆæ•°æ®æè¿°ä¸­çš„ç»Ÿè®¡ä¿¡æ¯å’Œåˆ†å¸ƒç‰¹å¾
                                    3. æ‰€æœ‰è¡¨æ ¼å¿…é¡»æ ¼å¼è§„èŒƒï¼Œæ•°æ®å¯¹é½
                                    4. é‡ç‚¹å…³æ³¨ç»Ÿè®¡æ˜¾è‘—çš„å…³ç³»å’Œå¼‚å¸¸å€¼
                                    5. ç¡®ä¿éæŠ€æœ¯äººå‘˜ä¹Ÿèƒ½ç†è§£æŠ¥å‘Šå†…å®¹
                                    6. æ‰€æœ‰ç»“è®ºå’Œå»ºè®®å¿…é¡»æœ‰æ•°æ®æ”¯æŒ
                                    """
                                }
                            ],
                            temperature=0.7,
                            max_tokens=5000,
                            stream=True
                        )
                        
                        full_response = ""
                        reasoning_content = ""
                        
                        # å¤„ç†æµå¼å“åº”
                        for chunk in response:
                            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                                reasoning_content += chunk.choices[0].delta.reasoning_content
                                dag_progress.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nåˆ†æç»“æœï¼š\n{full_response}")
                            elif hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                                full_response += chunk.choices[0].delta.content
                                dag_progress.markdown(f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nåˆ†æç»“æœï¼š\n{full_response}")
                        
                        # ä¿å­˜åˆ°session state
                        st.session_state.business_report = full_response + "\n\n# åˆ†æraw results\n" + dag_report + "\n\n# descriptive analysis\n" + json_output
                        st.session_state.dag_reasoning = reasoning_content
                        
                        # æ¸…ç©ºè¿›åº¦å®¹å™¨
                        dag_progress.empty()
                        
                except Exception as e:
                    st.error(f"DAGåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        
        # ä½¿ç”¨ä¸¤åˆ—æ˜¾ç¤ºåˆ†æç»“æœå’ŒDAGåˆ†æç»“æœ
        if st.session_state.analysis_response or st.session_state.business_report:
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### æ•°æ®åˆ†æ")
                if st.session_state.analysis_reasoning:
                    with st.expander("æ€è€ƒè¿‡ç¨‹", expanded=False):
                        st.markdown(st.session_state.analysis_reasoning)
                st.markdown(st.session_state.analysis_response)
                
                # DAGå®šä¹‰ç¼–è¾‘å™¨
                st.markdown("### DAGå®šä¹‰")
                dag_editor = st.text_area("ç¼–è¾‘DAGè¾¹å®šä¹‰:", value=st.session_state.dag_edges, height=200)
                if dag_editor != st.session_state.dag_edges:
                    st.session_state.dag_edges = dag_editor
                # Generate the Mermaid HTML
                try:
                    # ç¡®ä¿å…ˆè§£ææ–‡æœ¬ä¸­çš„dag_edges
                    dag_edges = extract_dag_edges(st.session_state.dag_edges)
                    if dag_edges:
                        mermaid_html = create_mermaid_html_from_edges(dag_edges)
                        # æ¸²æŸ“ Mermaid å›¾ï¼Œå¹¶å¢åŠ é«˜åº¦ä»¥é€‚åº”ç¼©æ”¾
                        st.markdown("## å…³ç³»å›¾ (ä½¿ç”¨å³ä¸Šè§’æŒ‰é’®ç¼©æ”¾)")
                        st.components.v1.html(mermaid_html, height=600, scrolling=True)
                    else:
                        st.warning("æ— æ³•è§£æDAGè¾¹å®šä¹‰ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
                except Exception as e:
                    st.error(f"ç”Ÿæˆå…³ç³»å›¾æ—¶å‡ºé”™: {str(e)}")
            
            with col2:
                if st.session_state.business_report:
                    st.markdown("### DAGåˆ†æç»“æœ")
                    if hasattr(st.session_state, 'dag_reasoning'):
                        with st.expander("åˆ†æè¿‡ç¨‹", expanded=False):
                            st.markdown(st.session_state.dag_reasoning)
                    st.markdown(st.session_state.business_report)
            
            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            if st.session_state.business_report:
                st.markdown("---")
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½åˆ†ææŠ¥å‘Š",
                    data=create_word_document(st.session_state.business_report),
                    file_name=f"data_analysis_report_{time.strftime('%Y%m%d_%H%M%S')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )

# Add this function before the main() function
def setup_sales_forecasting():
    st.markdown(
        """
    <h1 style='text-align: center; font-size: 18px; font-weight: bold;'>Sales Forecasting Application</h1>
    <h6 style='text-align: center; font-size: 12px;'>Upload your Excel file to forecast sales or other time series data</h6>
    <br><br><br>
    """,
        unsafe_allow_html=True,
    )

    np.random.seed(42)
    
    # Initialize session state for storing results between interactions
    if 'forecast_df' not in st.session_state:
        st.session_state.forecast_df = None
    if 'all_groups' not in st.session_state:
        st.session_state.all_groups = []
    if 'has_forecast' not in st.session_state:
        st.session_state.has_forecast = False
    if 'target_column' not in st.session_state:
        st.session_state.target_column = None
    if 'original_filename' not in st.session_state:
        st.session_state.original_filename = None
    if 'training_accuracy' not in st.session_state:
        st.session_state.training_accuracy = {}
    if 'validation_accuracy' not in st.session_state:
        st.session_state.validation_accuracy = {}
    if 'start_date_str' not in st.session_state:
        st.session_state.start_date_str = None
    if 'training_end_date_str' not in st.session_state:
        st.session_state.training_end_date_str = None
    
    st.markdown("""
    This app helps you forecast sales or other time series data using Facebook Prophet.
    Upload your Excel file, configure the settings, and get predictions for future periods.
    """)
    
    # File uploader
    uploaded_file = st.file_uploader("Upload Excel file", type=["xlsx", "xls"])
    
    if uploaded_file is not None:
        # Store original filename
        if st.session_state.original_filename is None:
            st.session_state.original_filename = uploaded_file.name.split('.')[0]
            
        # Load the file and display sheet selection
        excel_file = pd.ExcelFile(uploaded_file)
        sheet_name = st.selectbox("Select Sheet", excel_file.sheet_names)
        
        # Read the selected sheet
        df = pd.read_excel(uploaded_file, sheet_name=sheet_name)
        
        # Display data preview
        st.subheader("Data Preview")
        st.dataframe(df.head())
        
        # Basic data info
        st.subheader("Dataset Information")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"Number of rows: {df.shape[0]}")
        with col2:
            st.write(f"Number of columns: {df.shape[1]}")
        
        # Column selection and configuration
        st.subheader("Configure Forecasting Parameters")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Date column selection
            date_column_options = df.columns.tolist()
            date_column = st.selectbox("Select Date/Month Column", date_column_options)
            
            # Date format selection
            date_format_options = [
                "%Y%m", "%Y-%m", "%b %Y", "%B %Y", 
                "%Y%m%d", "%Y-%m-%d", "%d-%m-%Y", "%m/%d/%Y"
            ]
            date_format = st.selectbox("Select Date Format", date_format_options)
            
            # Target column (to be predicted)
            numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
            target_column = st.selectbox("Select Target Column to Forecast", numeric_columns)
            st.session_state.target_column = target_column
            
            # Start date for training
            min_date = pd.to_datetime("2020-01-01")  # Default minimum date
            max_date = datetime.now()
            start_date = st.date_input("Training Start Date", 
                                      value=pd.to_datetime("2022-01-01"),
                                      min_value=min_date,
                                      max_value=max_date)
            
        
        with col2:
            # Grouping columns (multi-select)
            all_columns = df.columns.tolist()
            grouping_columns = st.multiselect("Select Columns for Grouping (Optional)", all_columns)
    
            # Forecast horizon
            forecast_years = st.number_input("Forecast Horizon (Years)", min_value=1, max_value=10, value=3)
            
            # Add a forecast end date input
            current_date = datetime.now()
            default_end_date = current_date.replace(day=1) + pd.DateOffset(years=forecast_years) - pd.DateOffset(days=1)
            end_date = st.date_input("Forecast End Date", 
                                    value=default_end_date,
                                    min_value=current_date)
            
            # Hidden Confidence Interval Width (not displayed in UI)
            interval_width = 0.6  # Set default to 0.6 as requested
            
            # Add a training end date input at the bottom of col2
            training_end_date = st.date_input("Training End Date", 
                                    value=current_date,
                                    min_value=start_date,
                                    max_value=current_date)
        
        # Run forecast button
        if st.button("Run Forecast") or st.session_state.has_forecast:
            if not st.session_state.has_forecast:
                try:
                    # Progress bar
                    progress_bar = st.progress(0)
                    
                    # Copy the dataframe to avoid modifying the original
                    df_copy = df.copy()
                    
                    # Process date column
                    st.info("Processing date column...")
                    try:
                        df_copy['date'] = pd.to_datetime(df_copy[date_column].astype(str), format=date_format)
                    except:
                        df_copy['date'] = pd.to_datetime(df_copy[date_column].astype(str))
                        
                    # Store the original date format from the column
                    original_date_values = df_copy[date_column].copy()
                        
                    progress_bar.progress(10)
                    
                    # Create grouping key if group columns are selected
                    if grouping_columns:
                        st.info("Creating group combinations...")
                        df_copy['group'] = df_copy[grouping_columns[0]].astype(str)
                        for col in grouping_columns[1:]:
                            df_copy['group'] += '_' + df_copy[col].astype(str)
                    else:
                        # If no grouping is selected, create a single group
                        df_copy['group'] = 'all_data'
                        
                    progress_bar.progress(20)
                    
                    # Get current date and calculate end date for forecasting
                    current_date = datetime.now()
                    end_date_str = end_date.strftime('%Y-%m-%d')
                    training_end_date_str = training_end_date.strftime('%Y-%m-%d')
                    start_date_str = start_date.strftime('%Y-%m-%d')
                    
                    # Store date strings in session state for later use
                    st.session_state.start_date_str = start_date_str
                    st.session_state.training_end_date_str = training_end_date_str
                    
                    # Create complete date and group combinations
                    st.info("Creating complete date-group combinations...")
                    # ä¿®æ”¹: ç¡®ä¿ç”Ÿæˆçš„æ—¥æœŸèŒƒå›´å§‹ç»ˆå»¶ä¼¸åˆ°é¢„æµ‹ç»“æŸæ—¥æœŸ
                    all_dates = pd.date_range(start=df_copy['date'].min(), end=end_date_str, freq='MS')
                    all_groups = df_copy['group'].unique()
                    st.session_state.all_groups = all_groups.tolist()
                    
                    complete_df = pd.DataFrame([(date, group) for date in all_dates for group in all_groups],
                                              columns=['date', 'group'])
                    
                    # Merge with original data
                    df_copy = pd.merge(complete_df, df_copy, on=['date', 'group'], how='left')
                    
                    # Fill missing values for target column with 0
                    df_copy[target_column] = df_copy[target_column].fillna(0)
                    
                    # Forward fill other columns within groups
                    if grouping_columns:
                        for col in grouping_columns:
                            df_copy[col] = df_copy.groupby('group')[col].ffill().bfill()
                            
                    progress_bar.progress(40)
                    
                    # Sort data
                    df_copy = df_copy.sort_values(by=['group', 'date'], ascending=[True, True])
                    
                    # Convert dates to string format
                    current_date_str = current_date.strftime("%Y-%m-%d")
                    
                    # Run Prophet forecast for each group
                    st.info("Running forecasts for each group...")
                    forecasts = {}
                    training_accuracy = {}
                    validation_accuracy = {}
                    total_groups = len(all_groups)
                    
                    for i, group in enumerate(all_groups):
                        # Update progress based on group progress
                        progress_value = 40 + (i / total_groups * 50)
                        progress_bar.progress(int(progress_value))
                        
                        # Filter data for this group - using the start_date parameter
                        group_data = df_copy[(df_copy['group'] == group) & 
                                           (df_copy['date'] >= start_date_str) & 
                                           (df_copy['date'] <= training_end_date_str)].copy()
                        
                        group_data = group_data.rename(columns={'date': 'ds', target_column: 'y'})
                        
                        # Fit Prophet model
                        model = Prophet(interval_width=interval_width, uncertainty_samples=1000, mcmc_samples=300)
                        # model = Prophet(interval_width=interval_width)
                        try:
                            model.fit(group_data[['ds', 'y']])
                            
                            # ä¿®æ”¹: ç›´æ¥ä½¿ç”¨é¢„æµ‹ç»“æŸæ—¥æœŸæ¥åˆ›å»ºfuture dataframe
                            future_end_date = pd.to_datetime(end_date_str)
                            # åˆ›å»ºä»è®­ç»ƒæ•°æ®å¼€å§‹åˆ°é¢„æµ‹ç»“æŸæ—¥æœŸçš„å®Œæ•´æ—¥æœŸèŒƒå›´
                            future_dates = pd.date_range(start=group_data['ds'].min(), end=future_end_date, freq='MS')
                            future = pd.DataFrame({'ds': future_dates})
                            
                            # Make forecast
                            forecast = model.predict(future)
                            
                            forecasts[group] = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
                        except Exception as e:
                            st.warning(f"Could not forecast for group {group}: {str(e)}")
                            # Create empty forecast data for this group
                            forecasts[group] = pd.DataFrame({
                                'ds': all_dates,
                                'yhat': np.zeros(len(all_dates)),
                                'yhat_lower': np.zeros(len(all_dates)),
                                'yhat_upper': np.zeros(len(all_dates))
                            })
                    
                    progress_bar.progress(90)
                    
                    # Write forecasts back to the dataframe using direct array assignment
                    st.info("Combining results...")
                    for group, forecast in forecasts.items():
                        # Use date matching approach instead of direct array assignment
                        for i, row in forecast.iterrows():
                            forecast_date = row['ds']
                            # Match both group and date
                            mask = (df_copy['group'] == group) & (df_copy['date'] == forecast_date)
                            # Assign forecast values
                            df_copy.loc[mask, 'forecast'] = row['yhat']
                            df_copy.loc[mask, 'forecast_lower'] = row['yhat_lower']
                            df_copy.loc[mask, 'forecast_upper'] = row['yhat_upper']
                    
                    # Format dates in the original date column according to the selected format
                    # This preserves the original date format while extending to future dates
                    df_copy[date_column] = df_copy['date'].dt.strftime(date_format)
                    
                    # Calculate prediction accuracy percentages for each group
                    for group in all_groups:
                        # Training period accuracy (Training Start Date to Training End Date)
                        training_data = df_copy[(df_copy['group'] == group) & 
                                              (df_copy['date'] >= start_date_str) & 
                                              (df_copy['date'] <= training_end_date_str)]
                        
                        if not training_data.empty and training_data[target_column].sum() > 0:
                            actual = training_data[target_column].values
                            pred = training_data['forecast'].values
                            # Calculate percentage error: (pred - actual) / actual
                            pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                            # Calculate MAPE (Mean Absolute Percentage Error)
                            mean_pct_error = np.nanmean(pct_errors) * 100
                            training_accuracy[group] = mean_pct_error
                        else:
                            training_accuracy[group] = np.nan
                        
                        # Validation period accuracy (Training End Date to current date)
                        validation_data = df_copy[(df_copy['group'] == group) & 
                                                (df_copy['date'] > training_end_date_str) & 
                                                (df_copy['date'] <= current_date_str)]
                        
                        if not validation_data.empty and validation_data[target_column].sum() > 0:
                            actual = validation_data[target_column].values
                            pred = validation_data['forecast'].values
                            # Calculate percentage error: (pred - actual) / actual
                            pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                            # Calculate MAPE
                            mean_pct_error = np.nanmean(pct_errors) * 100
                            validation_accuracy[group] = mean_pct_error
                        else:
                            validation_accuracy[group] = np.nan
                    
                    # Store accuracy metrics in session state
                    st.session_state.training_accuracy = training_accuracy
                    st.session_state.validation_accuracy = validation_accuracy
                    
                    # Store results in session state
                    st.session_state.forecast_df = df_copy
                    st.session_state.has_forecast = True
                    
                    progress_bar.progress(100)
                    st.success("Forecast completed successfully!")
                    
                except Exception as e:
                    st.error(f"An error occurred during forecasting: {str(e)}")
                    st.error("Please check your data and settings and try again.")
            
            # Display results if available in session state
            if st.session_state.has_forecast and st.session_state.forecast_df is not None:
                # Display results
                st.subheader("Forecast Results")
                st.dataframe(st.session_state.forecast_df.head(20))
                
                # Create download link for the forecast
                st.subheader("Download Complete Forecast")
                
                # Get the original filename for downloads
                filename_base = st.session_state.original_filename
                
                # CSV download with custom filename
                csv = st.session_state.forecast_df.to_csv(index=False)
                b64 = base64.b64encode(csv.encode()).decode()
                csv_filename = f"{filename_base}_forecast.csv"
                href = f'<a href="data:file/csv;base64,{b64}" download="{csv_filename}">Download CSV File</a>'
                st.markdown(href, unsafe_allow_html=True)
                
                # Excel download with custom filename
                buffer = io.BytesIO()
                st.session_state.forecast_df.to_excel(buffer, index=False)
                buffer.seek(0)
                b64_excel = base64.b64encode(buffer.read()).decode()
                excel_filename = f"{filename_base}_forecast.xlsx"
                href_excel = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64_excel}" download="{excel_filename}">Download Excel File</a>'
                st.markdown(href_excel, unsafe_allow_html=True)
                
                # Visualization section
                st.subheader("Forecast Visualizations")
                
                # Create dropdown for group selection
                selected_group = st.selectbox("Select group to visualize", st.session_state.all_groups)
                
                # Get accuracy metrics for selected group
                training_acc = st.session_state.training_accuracy.get(selected_group, np.nan)
                validation_acc = st.session_state.validation_accuracy.get(selected_group, np.nan)
                
                # Format accuracy metrics for display
                training_acc_text = f"Training Accuracy: {training_acc:.2f}%" if not np.isnan(training_acc) else "Training Accuracy: N/A"
                validation_acc_text = f"Validation Accuracy: {validation_acc:.2f}%" if not np.isnan(validation_acc) else "Validation Accuracy: N/A"
                
                # Plot the selected group
                selected_data = st.session_state.forecast_df[st.session_state.forecast_df['group'] == selected_group].copy()
                
                # ä¿®æ”¹: ç¡®ä¿ä½¿ç”¨æ’åºåçš„æ•°æ®æ¥åˆ›å»ºå›¾è¡¨ï¼Œé¿å…confidence intervalçš„æ˜¾ç¤ºé—®é¢˜
                selected_data = selected_data.sort_values('date')
                
                # Create plotly figure for selected group
                fig = make_subplots(specs=[[{"secondary_y": True}]])
                
                # Add actual values
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data[st.session_state.target_column],
                        mode='lines+markers',
                        name='Actual',
                        line=dict(color='blue')
                    )
                )
                
                # Add forecast
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data['forecast'],
                        mode='lines',
                        name='Forecast',
                        line=dict(color='red')
                    )
                )
                
                # ä¿®æ”¹: æ›´æ”¹confidence intervalçš„ç»˜åˆ¶æ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®ç»˜åˆ¶
                # æ·»åŠ ä¸Šè¾¹ç•Œ
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data['forecast_upper'],
                        mode='lines',
                        line=dict(width=0),
                        showlegend=False
                    )
                )
                
                # æ·»åŠ ä¸‹è¾¹ç•Œ
                fig.add_trace(
                    go.Scatter(
                        x=selected_data['date'],
                        y=selected_data['forecast_lower'],
                        mode='lines',
                        line=dict(width=0),
                        fillcolor='rgba(255,0,0,0.2)',
                        fill='tonexty',
                        name='Confidence Interval'
                    )
                )
                
                # Update layout with accuracy metrics in the title
                fig.update_layout(
                    title=f'Forecast for {selected_group} ({training_acc_text}, {validation_acc_text})',
                    xaxis_title='Date',
                    yaxis_title=st.session_state.target_column,
                    hovermode='x unified',
                    legend=dict(orientation='h', y=1.1)
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Add option to view overall forecast (sum of all groups)
                if st.checkbox("View Overall Forecast (Sum of All Groups)"):
                    # Group by date and sum the values
                    overall_data = st.session_state.forecast_df.groupby('date').agg({
                        st.session_state.target_column: 'sum',
                        'forecast': 'sum',
                        'forecast_lower': 'sum',
                        'forecast_upper': 'sum'
                    }).reset_index()
                    
                    # ä¿®æ”¹: ç¡®ä¿ä½¿ç”¨æ’åºåçš„æ•°æ®æ¥åˆ›å»ºå›¾è¡¨ï¼Œé¿å…confidence intervalçš„æ˜¾ç¤ºé—®é¢˜
                    overall_data = overall_data.sort_values('date')
                    
                    # Calculate overall accuracy metrics
                    # Convert dates to datetime for comparison
                    overall_data['date_dt'] = pd.to_datetime(overall_data['date'])
                    
                    # Use stored date strings from session state
                    start_date_str = st.session_state.start_date_str
                    training_end_date_str = st.session_state.training_end_date_str
                    
                    # Convert to datetime objects for comparison
                    start_date_dt = pd.to_datetime(start_date_str)
                    training_end_date_dt = pd.to_datetime(training_end_date_str)
                    current_date_dt = pd.to_datetime(current_date.strftime("%Y-%m-%d"))
                    
                    # Training period accuracy (Training Start Date to Training End Date)
                    training_overall = overall_data[(overall_data['date_dt'] >= start_date_dt) & 
                                                  (overall_data['date_dt'] <= training_end_date_dt)]
                    
                    if not training_overall.empty and training_overall[st.session_state.target_column].sum() > 0:
                        actual = training_overall[st.session_state.target_column].values
                        pred = training_overall['forecast'].values
                        # Calculate percentage error: (pred - actual) / actual
                        pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                        # Calculate MAPE
                        overall_training_acc = np.nanmean(pct_errors) * 100
                    else:
                        overall_training_acc = np.nan
                    
                    # Validation period accuracy (Training End Date to current date)
                    validation_overall = overall_data[(overall_data['date_dt'] > training_end_date_dt) & 
                                                    (overall_data['date_dt'] <= current_date_dt)]
                    
                    if not validation_overall.empty and validation_overall[st.session_state.target_column].sum() > 0:
                        actual = validation_overall[st.session_state.target_column].values
                        pred = validation_overall['forecast'].values
                        # Calculate percentage error: (pred - actual) / actual
                        pct_errors = np.where(actual > 0, (pred - actual) / actual, np.nan)
                        # Calculate MAPE
                        overall_validation_acc = np.nanmean(pct_errors) * 100
                    else:
                        overall_validation_acc = np.nan
                    
                    # Format overall accuracy metrics for display
                    overall_training_acc_text = f"Training Accuracy: {overall_training_acc:.2f}%" if not np.isnan(overall_training_acc) else "Training Accuracy: N/A"
                    overall_validation_acc_text = f"Validation Accuracy: {overall_validation_acc:.2f}%" if not np.isnan(overall_validation_acc) else "Validation Accuracy: N/A"
                    
                    # Create plotly figure for overall data
                    fig_overall = make_subplots(specs=[[{"secondary_y": True}]])
                    
                    # Add actual values
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data[st.session_state.target_column],
                            mode='lines+markers',
                            name='Actual',
                            line=dict(color='blue')
                        )
                    )
                    
                    # Add forecast
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data['forecast'],
                            mode='lines',
                            name='Forecast',
                            line=dict(color='red')
                        )
                    )
                    
                    # ä¿®æ”¹: æ›´æ”¹confidence intervalçš„ç»˜åˆ¶æ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®ç»˜åˆ¶
                    # æ·»åŠ ä¸Šè¾¹ç•Œ
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data['forecast_upper'],
                            mode='lines',
                            line=dict(width=0),
                            showlegend=False
                        )
                    )
                    
                    # æ·»åŠ ä¸‹è¾¹ç•Œ
                    fig_overall.add_trace(
                        go.Scatter(
                            x=overall_data['date'],
                            y=overall_data['forecast_lower'],
                            mode='lines',
                            line=dict(width=0),
                            fillcolor='rgba(255,0,0,0.2)',
                            fill='tonexty',
                            name='Confidence Interval'
                        )
                    )
                    
                    # Update layout with accuracy metrics in the title
                    fig_overall.update_layout(
                        title=f'Overall Forecast (Sum of All Groups) ({overall_training_acc_text}, {overall_validation_acc_text})',
                        xaxis_title='Date',
                        yaxis_title=st.session_state.target_column,
                        hovermode='x unified',
                        legend=dict(orientation='h', y=1.1)
                    )
                    
                    st.plotly_chart(fig_overall, use_container_width=True)
        
        # Add button to clear results and start over
        if st.session_state.has_forecast:
            if st.button("Clear Results and Start Over"):
                # Fix for experimental_rerun issue - use session state to clear data
                st.session_state.forecast_df = None
                st.session_state.all_groups = []
                st.session_state.has_forecast = False
                st.session_state.target_column = None
                st.session_state.original_filename = None
                st.session_state.training_accuracy = {}
                st.session_state.validation_accuracy = {}
                st.session_state.start_date_str = None
                st.session_state.training_end_date_str = None
                # Use Streamlit's rerun function instead of experimental_rerun
                st.rerun()
    
    else:
        # Display sample instructions when no file is uploaded
        st.info("Please upload an Excel file to begin. Your file should contain:")
        st.markdown("""
        - A column with dates or year-month values
        - At least one numeric column to forecast
        - Optional grouping columns (e.g., product codes, regions)
        
        The app will:
        1. Process your data
        2. Fill missing values
        3. Generate forecasts using Facebook Prophet
        4. Allow you to download the complete forecast results
        """)
        
        # Sample data structure
        st.subheader("Sample Data Structure")
        sample_data = pd.DataFrame({
            'year_month': ['202201', '202202', '202203', '202201', '202202', '202203'],
            'product_code': ['A001', 'A001', 'A001', 'B002', 'B002', 'B002'],
            'region': ['North', 'North', 'North', 'South', 'South', 'South'],
            'sales_quantity': [100, 120, 110, 80, 85, 95],
            'sales_amount': [5000, 6000, 5500, 4000, 4250, 4750]
        })
        
        st.dataframe(sample_data)
    
    # Footer
    st.markdown("""
    ---
    ### Notes:
    - For best results, provide at least 12 months of historical data
    - If using grouping, ensure all groups have sufficient data points
    - The forecast horizon can be adjusted based on your needs
    """)


# Modify the main() function to add the Sales Forecasting option
def main():
    st.set_page_config(layout="wide")
    
    # Create the page selection in sidebar
    page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["Medical Insights Copilot", "Spreadsheet Analysis", "Sales Forecasting"])
    
    if page == "Medical Insights Copilot":  
        model_choice, client = setup_client()
        setup_layout(
            topics, diseases, institutions, departments, persons,
            primary_topics_list, primary_diseases_list,
            generate_tag, generate_diseases_tag, rewrite,
            prob_identy, generate_structure_data,
            model_choice, client
        )
    elif page == "Spreadsheet Analysis":
        setup_spreadsheet_analysis()
    elif page == "Sales Forecasting":
        setup_sales_forecasting()


if __name__ == "__main__":
    main()
